@book{koller2009probabilistic,
  title={Probabilistic graphical models: principles and techniques},
  author={Koller, Daphne and Friedman, Nir},
  year={2009},
  publisher={MIT press}
}

@article{ccatal2020learning,
  title={Learning generative state space models for active inference},
  author={{\c{C}}atal, Ozan and Wauthier, Samuel and De Boom, Cedric and Verbelen, Tim and Dhoedt, Bart},
  journal={Frontiers in Computational Neuroscience},
  volume={14},
  pages={574372},
  year={2020},
  publisher={Frontiers Media SA}
}

@article{DAI_HR,
  title={Robot self/other distinction: active inference meets neural networks learning in a mirror},
  author={Lanillos, Pablo and Cheng, Gordon and others},
  journal={arXiv preprint arXiv:2004.05473},
  year={2020}
}

@article{DAI_HR2,
  title={Active inference body perception and action for humanoid robots},
  author={Oliver, Guillermo and Lanillos, Pablo and Cheng, Gordon},
  journal={arXiv preprint arXiv:1906.03022},
  year={2019}
}

@article{DAI_POMDP,
  author    = {Otto van der Himst and
               Pablo Lanillos},
  title     = {Deep Active Inference for Partially Observable MDPs},
  journal   = {CoRR},
  volume    = {abs/2009.03622},
  year      = {2020},
  url       = {https://arxiv.org/abs/2009.03622},
  eprinttype = {arXiv},
  eprint    = {2009.03622},
  timestamp = {Thu, 17 Sep 2020 12:49:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2009-03622.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{schneider2022active,
  title={Active Inference for Robotic Manipulation},
  author={Schneider, Tim and Belousov, Boris and Abdulsamad, Hany and Peters, Jan},
  journal={arXiv preprint arXiv:2206.10313},
  year={2022}
}

@article{DAI_Kai,
author = {Ueltzh\"{o}ffer, Kai},
title = {Deep Active Inference},
year = {2018},
issue_date = {December 2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {112},
number = {6},
issn = {0340-1200},
url = {https://doi.org/10.1007/s00422-018-0785-7},
doi = {10.1007/s00422-018-0785-7},
abstract = {This work combines the free energy principle and the ensuing active inference dynamics with recent advances in variational inference in deep generative models, and evolution strategies to introduce the "deep active inference" agent. This agent minimises a variational free energy bound on the average surprise of its sensations, which is motivated by a homeostatic argument. It does so by optimising the parameters of a generative latent variable model of its sensory inputs, together with a variational density approximating the posterior distribution over the latent variables, given its observations, and by acting on its environment to actively sample input that is likely under this generative model. The internal dynamics of the agent are implemented using deep and recurrent neural networks, as used in machine learning, making the deep active inference agent a scalable and very flexible class of active inference agent. Using the mountain car problem, we show how goal-directed behaviour can be implemented by defining appropriate priors on the latent states in the agent's model. Furthermore, we show that the deep active inference agent can learn a generative model of the environment, which can be sampled from to understand the agent's beliefs about the environment and its interaction therewith.},
journal = {Biol. Cybern.},
month = {dec},
pages = {547–573},
numpages = {27},
keywords = {Deep learning, Perception, Variational inference, Cognition, Generative models, Action}
}

@InProceedings{rood2020deep,
author="Rood, Thomas
and van Gerven, Marcel
and Lanillos, Pablo",
editor="Verbelen, Tim
and Lanillos, Pablo
and Buckley, Christopher L.
and De Boom, Cedric",
title="A Deep Active Inference Model of the Rubber-Hand Illusion",
booktitle="Active Inference",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="84--91",
abstract="Understanding how perception and action deal with sensorimotor conflicts, such as the rubber-hand illusion (RHI), is essential to understand how the body adapts to uncertain situations. Recent results in humans have shown that the RHI not only produces a change in the perceived arm location, but also causes involuntary forces. Here, we describe a deep active inference agent in a virtual environment, which we subjected to the RHI, that is able to account for these results. We show that our model, which deals with visual high-dimensional inputs, produces similar perceptual and force patterns to those found in humans.",
isbn="978-3-030-64919-7"
}

@ARTICLE{MCTS,
  author={C. B. {Browne} and E. {Powley} and D. {Whitehouse} and S. M. {Lucas} and P. I. {Cowling} and P. {Rohlfshagen} and S. {Tavener} and D. {Perez} and S. {Samothrakis} and S. {Colton}},
  journal={IEEE Transactions on Computational Intelligence and AI in Games}, 
  title={A Survey of Monte Carlo Tree Search Methods}, 
  year={2012},
  volume={4},
  number={1},
  pages={1-43}
}

@book{sutton1998,
  title={Introduction to reinforcement learning},
  author={Sutton, Richard S and Barto, Andrew G and others},
  year={1998},
  publisher={MIT press Cambridge}
}

@misc{dsprites17,
author = {Loic Matthey and Irina Higgins and Demis Hassabis and Alexander Lerchner},
title = {dSprites: Disentanglement testing Sprites dataset},
howpublished= {https://github.com/deepmind/dsprites-dataset/},
year = "2017",
}

@misc{AITS_THEORY,
      title={Branching Time Active Inference: the theory and its generality}, 
      author={Th{\'{e}}ophile Champion and Howard Bowman and Marek Grze{\'{s}}},
      year={2021},
      archivePrefix={arXiv}
}

@misc{AITS_PRACTICE,
      title={Branching Time Active Inference: empirical study}, 
      author={Th{\'{e}}ophile Champion and Howard Bowman and Marek Grze{\'{s}}},
      year={2021},
      archivePrefix={arXiv}
}

@misc{BTAI_BF,
      title={{Branching Time Active Inference with Bayesian Filtering}}, 
      author={Th{\'{e}}ophile Champion and Marek Grze{\'{s}} and Howard Bowman},
      year={2021},
      archivePrefix={arXiv}
}

@Article{Auer2002,
author={Auer, Peter and Cesa-Bianchi, Nicol{\`o} and Fischer, Paul},
title={Finite-time Analysis of the Multiarmed Bandit Problem},
journal={Machine Learning},
year={2002},
month={May},
day={01},
volume={47},
number={2},
pages={235-256},
abstract={Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
issn={1573-0565},
doi={10.1023/A:1013689704352},
url={https://doi.org/10.1023/A:1013689704352}
}

@article{Go,
  author    = {David Silver and
               Aja Huang and
               Chris J. Maddison and
               Arthur Guez and
               Laurent Sifre and
               George van den Driessche and
               Julian Schrittwieser and
               Ioannis Antonoglou and
               Vedavyas Panneershelvam and
               Marc Lanctot and
               Sander Dieleman and
               Dominik Grewe and
               John Nham and
               Nal Kalchbrenner and
               Ilya Sutskever and
               Timothy P. Lillicrap and
               Madeleine Leach and
               Koray Kavukcuoglu and
               Thore Graepel and
               Demis Hassabis},
  title     = {Mastering the game of Go with deep neural networks and tree search},
  journal   = {Nature},
  volume    = {529},
  number    = {7587},
  pages     = {484--489},
  year      = {2016},
  url       = {https://doi.org/10.1038/nature16961},
  doi       = {10.1038/nature16961},
  timestamp = {Wed, 14 Nov 2018 10:30:42 +0100},
  biburl    = {https://dblp.org/rec/journals/nature/SilverHMGSDSAPL16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{AI_VMP,
      title={Realising Active Inference in Variational Message Passing: the Outcome-blind Certainty Seeker}, 
      author={Th{\'{e}}ophile Champion and Marek Grze{\'{s}} and Howard Bowman},
      year={2021},
      archivePrefix={arXiv}
}

@misc{DeepAIwithMCMC,
    title={Deep active inference agents using {Monte-Carlo} methods},
    author={Zafeirios Fountas and Noor Sajid and Pedro A. M. Mediano and Karl Friston},
    year={2020},
    eprint={2006.04176},
    archivePrefix={arXiv},
    primaryClass={q-bio.NC}
}

@article{FRISTON2016862,
	title = "Active inference and learning",
	journal = "Neuroscience \& Biobehavioral Reviews",
	volume = "68",
	pages = "862 - 879",
	year = "2016",
	issn = "0149-7634",
	doi = "https://doi.org/10.1016/j.neubiorev.2016.06.022",
	author = "Karl Friston and Thomas FitzGerald and Francesco Rigoli and Philipp Schwartenbeck and John O Doherty and Giovanni Pezzulo",
	keywords = "Active inference, Habit learning, Bayesian inference, Goal-directed, Free energy, Information gain, Bayesian surprise, Epistemic value, Exploration, Exploitation",
}

@misc{AI_TUTO,
    title={Active inference on discrete state-spaces: a synthesis},
    author={Lancelot Da Costa and Thomas Parr and Noor Sajid and Sebastijan Veselic and Victorita Neacsu and Karl Friston},
    year={2020},
    eprint={2001.07203},
    archivePrefix={arXiv},
    primaryClass={q-bio.NC}
}

@article{bayes_surprise,
title = "Bayesian surprise attracts human attention",
journal = "Vision Research",
volume = "49",
number = "10",
pages = "1295 - 1306",
year = "2009",
note = "Visual Attention: Psychophysics, electrophysiology and neuroimaging",
issn = "0042-6989",
doi = "https://doi.org/10.1016/j.visres.2008.09.007",
url = "http://www.sciencedirect.com/science/article/pii/S0042698908004380",
author = "Laurent Itti and Pierre Baldi",
keywords = "Attention, Surprise, Bayes theorem, Information theory, Eye movements, Natural vision, Free viewing, Saliency, Novelty",
abstract = "We propose a formal Bayesian definition of surprise to capture subjective aspects of sensory information. Surprise measures how data affects an observer, in terms of differences between posterior and prior beliefs about the world. Only data observations which substantially affect the observer’s beliefs yield surprise, irrespectively of how rare or informative in Shannon’s sense these observations are. We test the framework by quantifying the extent to which humans may orient attention and gaze towards surprising events or items while watching television. To this end, we implement a simple computational model where a low-level, sensory form of surprise is computed by simple simulated early visual neurons. Bayesian surprise is a strong attractor of human attention, with 72% of all gaze shifts directed towards locations more surprising than the average, a figure rising to 84% when focusing the analysis onto regions simultaneously selected by all observers. The proposed theory of surprise is applicable across different spatio-temporal scales, modalities, and levels of abstraction."
}

@article {curiosity,
	author = {Schwartenbeck, Philipp and Passecker, Johannes and Hauser, Tobias U and FitzGerald, Thomas H B and Kronbichler, Martin and Friston, Karl},
	title = {Computational mechanisms of curiosity and goal-directed exploration},
	elocation-id = {411272},
	year = {2018},
	doi = {10.1101/411272},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Successful behaviour depends on the right balance between maximising reward and soliciting information about the world. Here, we show how different types of information-gain emerge when casting behaviour as surprise minimisation. We present two distinct mechanisms for goal-directed exploration that express separable profiles of active sampling to reduce uncertainty. {\textquoteleft}Hidden state{\textquoteright} exploration motivates agents to sample unambiguous observations to accurately infer the (hidden) state of the world. Conversely, {\textquoteleft}model parameter{\textquoteright} exploration, compels agents to sample outcomes associated with high uncertainty, if they are informative for their representation of the task structure. We illustrate the emergence of these types of information-gain, termed active inference and active learning, and show how these forms of exploration induce distinct patterns of {\textquoteleft}Bayes-optimal{\textquoteright} behaviour. Our findings provide a computational framework to understand how distinct levels of uncertainty induce different modes of information-gain in decision-making.},
	URL = {https://www.biorxiv.org/content/early/2018/09/07/411272},
	eprint = {https://www.biorxiv.org/content/early/2018/09/07/411272.full.pdf},
	journal = {bioRxiv}
}

@ARTICLE{dopamine,
AUTHOR={FitzGerald, Thomas H. B. and Dolan, Raymond J. and Friston, Karl},
TITLE={Dopamine, reward learning, and active inference},      
JOURNAL={Frontiers in Computational Neuroscience},
VOLUME={9},
PAGES={136},
YEAR={2015},
URL={https://www.frontiersin.org/article/10.3389/fncom.2015.00136},
DOI={10.3389/fncom.2015.00136},
ISSN={1662-5188},   
ABSTRACT={Temporal difference learning models propose phasic dopamine signaling encodes reward prediction errors that drive learning. This is supported by studies where optogenetic stimulation of dopamine neurons can stand in lieu of actual reward. Nevertheless, a large body of data also shows that dopamine is not necessary for learning, and that dopamine depletion primarily affects task performance. We offer a resolution to this paradox based on an hypothesis that dopamine encodes the precision of beliefs about alternative actions, and thus controls the outcome-sensitivity of behavior. We extend an active inference scheme for solving Markov decision processes to include learning, and show that simulated dopamine dynamics strongly resemble those actually observed during instrumental conditioning. Furthermore, simulated dopamine depletion impairs performance but spares learning, while simulated excitation of dopamine neurons drives reward learning, through aberrant inference about outcome states. Our formal approach provides a novel and parsimonious reconciliation of apparently divergent experimental findings.}
}

@article{PAI,
title = "Planning as inference",
journal = "Trends in Cognitive Sciences",
volume = "16",
number = "10",
pages = "485 - 488",
year = "2012",
issn = "1364-6613",
doi = "https://doi.org/10.1016/j.tics.2012.08.006",
author = "Matthew Botvinick and Marc Toussaint",
}

@misc{pezzato2020active,
      title={Active Inference and Behavior Trees for Reactive Action Planning and Execution in Robotics}, 
      author={Corrado Pezzato and Carlos Hernandez and Martijn Wisse},
      year={2020},
      eprint={2011.09756},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@misc{sancaktar2020endtoend,
      title={End-to-End Pixel-Based Deep Active Inference for Body Perception and Action}, 
      author={Cansu Sancaktar and Marcel van Gerven and Pablo Lanillos},
      year={2020},
      eprint={2001.05847},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{CULLEN2018809,
title = "Active Inference in OpenAI Gym: A Paradigm for Computational Investigations Into Psychiatric Illness",
journal = "Biological Psychiatry: Cognitive Neuroscience and Neuroimaging",
volume = "3",
number = "9",
pages = "809 - 818",
year = "2018",
note = "Computational Methods and Modeling in Psychiatry",
issn = "2451-9022",
doi = "https://doi.org/10.1016/j.bpsc.2018.06.010",
url = "http://www.sciencedirect.com/science/article/pii/S2451902218301617",
author = "Maell Cullen and Ben Davey and Karl J. Friston and Rosalyn J. Moran",
keywords = "Active inference, Computational phenotyping, Computational psychiatry, Free energy principle, Game-based imaging biomarkers, Markov decision process",
abstract = "Background
Artificial intelligence has recently attained humanlike performance in a number of gamelike domains. These advances have been spurred by brain-inspired architectures and algorithms such as hierarchical filtering and reinforcement learning. OpenAI Gym is an open-source platform in which to train, test, and benchmark algorithms—it provides a range of tasks, including those of classic arcade games such as Doom. Here we describe how the platform might be used as a simulation, test, and diagnostic paradigm for psychiatric conditions.
Methods
To illustrate how active inference models of game play could be used to test mechanistic and algorithmic properties of psychiatric disorders, we provide two exemplar analyses. The first speaks to the impact of aging on cognition, examining game-play behaviors in a model of aging in which we compared age-dependent changes of younger (n = 9, 22 ± 1 years of age) and older (n = 7, 56 ± 5 years of age) adult players. The second is an illustration of a putative feature of anhedonia in which we simulated diminished sensitivity to reward.
Results
These simulations demonstrate how active inference can be used to test predicted changes in both neurobiology and beliefs in psychiatric cohorts. We show that, as well as behavioral measures, putative neural correlates of active inference can be simulated, and hypothesized (model-based) differences in local field potentials and blood oxygen level–dependent responses can be produced.
Conclusions
We show that active inference, through epistemic and value-based goals, enables simulated subjects to actively develop detailed representations of gaming environments, and we demonstrate the use of a principled algorithmic and neurobiological framework for testing hypotheses in psychiatric illness."
}

@misc{cart_pole,
      author={Beren Millidge},
      title={Combining Active Inference and Hierarchical Predictive Coding: A Tutorial Introduction and Case Study.}, 
      year={2019},
      url = {https://doi.org/10.31234/osf.io/kf6wc}
}

@Article{VI_TUTO,
author={Fox, Charles W.
and Roberts, Stephen J.},
title={A tutorial on variational Bayesian inference},
journal={Artificial Intelligence Review},
year={2012},
month={Aug},
day={01},
volume={38},
number={2},
pages={85-95},
abstract={This tutorial describes the mean-field variational Bayesian approximation to inference in graphical models, using modern machine learning terminology rather than statistical physics concepts. It begins by seeking to find an approximate mean-field distribution close to the target joint in the KL-divergence sense. It then derives local node updates and reviews the recent Variational Message Passing framework.},
issn={1573-7462},
doi={10.1007/s10462-011-9236-8},
url={https://doi.org/10.1007/s10462-011-9236-8}
}

@Article{VMP_TUTO,
author = { John Winn and Christopher Bishop },
title = {Variational Message Passing},
journal = {Journal of Machine Learning Research},
publisher = {Microtome publishing},
year = 2005,
volume = {6},
pages = {661-694}
}

@ARTICLE{FFG_TUTO,
  author={G. D. {Forney}},
  journal={IEEE Transactions on Information Theory}, 
  title={Codes on graphs: normal realizations}, 
  year={2001},
  volume={47},
  number={2},
  pages={520-548}
}

@article{Simul_AI,
  author    = {Thijs van de Laar and
               Bert de Vries},
  title     = {Simulating Active Inference Processes by Message Passing},
  journal   = {Front. Robotics and {AI}},
  volume    = {2019},
  year      = {2019},
  url       = {https://doi.org/10.3389/frobt.2019.00020},
  doi       = {10.3389/frobt.2019.00020},
  timestamp = {Tue, 16 Apr 2019 16:32:33 +0200},
  biburl    = {https://dblp.org/rec/journals/firai/LaarV19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/ijar/CoxLV19,
  author    = {Marco Cox and
               Thijs van de Laar and
               Bert de Vries},
  title     = {A factor graph approach to automated design of {Bayesian} signal processing
               algorithms},
  journal   = {Int. J. Approx. Reason.},
  volume    = {104},
  pages     = {185--204},
  year      = {2019},
  url       = {https://doi.org/10.1016/j.ijar.2018.11.002},
  doi       = {10.1016/j.ijar.2018.11.002},
  timestamp = {Fri, 21 Feb 2020 13:12:23 +0100},
  biburl    = {https://dblp.org/rec/journals/ijar/CoxLV19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{6145622,
  author={C. B. {Browne} and E. {Powley} and D. {Whitehouse} and S. M. {Lucas} and P. I. {Cowling} and P. {Rohlfshagen} and S. {Tavener} and D. {Perez} and S. {Samothrakis} and S. {Colton}},
  journal={IEEE Transactions on Computational Intelligence and AI in Games}, 
  title={A Survey of Monte Carlo Tree Search Methods}, 
  year={2012},
  volume={4},
  number={1},
  pages={1-43}
}

@article{MuZero,
  title={Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model},
  author={Julian Schrittwieser and Ioannis Antonoglou and Thomas Hubert and Karen Simonyan and Laurent Sifre and Simon Schmitt and Arthur Guez and Edward Lockhart and Demis Hassabis and Thore Graepel and Timothy P. Lillicrap and David Silver},
  journal={ArXiv},
  year={2019},
  volume={abs/1911.08265}
}

@misc{BTAI_3MF,
  doi = {10.48550/ARXIV.2206.12503},
  url = {https://arxiv.org/abs/2206.12503},
  author = {Champion, Théophile and Grześ, Marek and Bowman, Howard},
  keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Multi-Modal and Multi-Factor Branching Time Active Inference},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{dacosta2020relationship,
      title={The relationship between dynamic programming and active inference: the discrete, finite-horizon case}, 
      author={Lancelot Da Costa and Noor Sajid and Thomas Parr and Karl Friston and Ryan Smith},
      year={2020},
      eprint={2009.08111},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@ARTICLE{10.3389/fncom.2015.00136,
AUTHOR={FitzGerald, Thomas H. B. and Dolan, Raymond J. and Friston, Karl},   
TITLE={Dopamine, reward learning, and active inference},      
JOURNAL={Frontiers in Computational Neuroscience},      
VOLUME={9},      
PAGES={136},     
YEAR={2015},      
URL={https://www.frontiersin.org/article/10.3389/fncom.2015.00136},       
DOI={10.3389/fncom.2015.00136},      
ISSN={1662-5188},   
ABSTRACT={Temporal difference learning models propose phasic dopamine signaling encodes reward prediction errors that drive learning. This is supported by studies where optogenetic stimulation of dopamine neurons can stand in lieu of actual reward. Nevertheless, a large body of data also shows that dopamine is not necessary for learning, and that dopamine depletion primarily affects task performance. We offer a resolution to this paradox based on an hypothesis that dopamine encodes the precision of beliefs about alternative actions, and thus controls the outcome-sensitivity of behavior. We extend an active inference scheme for solving Markov decision processes to include learning, and show that simulated dopamine dynamics strongly resemble those actually observed during instrumental conditioning. Furthermore, simulated dopamine depletion impairs performance but spares learning, while simulated excitation of dopamine neurons drives reward learning, through aberrant inference about outcome states. Our formal approach provides a novel and parsimonious reconciliation of apparently divergent experimental findings.}
}

@ARTICLE{BAYESIAN_FILTERING,  author={Fox, V. and Hightower, J. and Lin Liao and Schulz, D. and Borriello, G.},  journal={IEEE Pervasive Computing},   title={Bayesian filtering for location estimation},   year={2003},  volume={2},  number={3},  pages={24-33},  doi={10.1109/MPRV.2003.1228524}}

@ARTICLE{AI_DISCRET,
AUTHOR={Friston, Karl and Schwartenbeck, Philipp and Fitzgerald, Thomas and Moutoussis, Michael and Behrens, Tim and Dolan, Raymond},   
TITLE={The anatomy of choice: active inference and agency},      
JOURNAL={Frontiers in Human Neuroscience},      
VOLUME={7},     
PAGES={598},    
YEAR={2013},      
URL={https://www.frontiersin.org/article/10.3389/fnhum.2013.00598},       
DOI={10.3389/fnhum.2013.00598},      
ISSN={1662-5161},   
ABSTRACT={This paper considers agency in the setting of embodied or active inference. In brief, we associate a sense of agency with prior beliefs about action and ask what sorts of beliefs underlie optimal behavior. In particular, we consider prior beliefs that action minimizes the Kullback–Leibler (KL) divergence between desired states and attainable states in the future. This allows one to formulate bounded rationality as approximate Bayesian inference that optimizes a free energy bound on model evidence. We show that constructs like expected utility, exploration bonuses, softmax choice rules and optimism bias emerge as natural consequences of this formulation. Previous accounts of active inference have focused on predictive coding and Bayesian filtering schemes for minimizing free energy. Here, we consider variational Bayes as an alternative scheme that provides formal constraints on the computational anatomy of inference and action—constraints that are remarkably consistent with neuroanatomy. Furthermore, this scheme contextualizes optimal decision theory and economic (utilitarian) formulations as pure inference problems. For example, expected utility theory emerges as a special case of free energy minimization, where the sensitivity or inverse temperature (of softmax functions and quantal response equilibria) has a unique and Bayes-optimal solution—that minimizes free energy. This sensitivity corresponds to the precision of beliefs about behavior, such that attainable goals are afforded a higher precision or confidence. In turn, this means that optimal behavior entails a representation of confidence about outcomes that are under an agent's control.}
}

@article {Parr304782,
	author = {Parr, Thomas and Friston, Karl J},
	title = {Generalised free energy and active inference: can the future cause the past?},
	elocation-id = {304782},
	year = {2018},
	doi = {10.1101/304782},
	publisher = {Cold Spring Harbor Laboratory},
	URL = {https://www.biorxiv.org/content/early/2018/04/23/304782},
	eprint = {https://www.biorxiv.org/content/early/2018/04/23/304782.full.pdf},
	journal = {bioRxiv}
}

@misc{millidge2020expected,
      title={Whence the Expected Free Energy?}, 
      author={Beren Millidge and Alexander Tschantz and Christopher L Buckley},
      year={2020},
      eprint={2004.08128},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{DeepAI,
title = "Deep active inference as variational policy gradients",
journal = "Journal of Mathematical Psychology",
volume = "96",
pages = "102348",
year = "2020",
issn = "0022-2496",
doi = "https://doi.org/10.1016/j.jmp.2020.102348",
url = "http://www.sciencedirect.com/science/article/pii/S0022249620300298",
author = "Beren Millidge",
keywords = "Active inference, Predictive processing, Neural networks, Policy gradients, Reinforcement learning",
}

@article{DBLP:journals/corr/abs-1801-01290,
  author    = {Tuomas Haarnoja and
               Aurick Zhou and
               Pieter Abbeel and
               Sergey Levine},
  title     = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning
               with a Stochastic Actor},
  journal   = {CoRR},
  volume    = {abs/1801.01290},
  year      = {2018},
  url       = {http://arxiv.org/abs/1801.01290},
  archivePrefix = {arXiv},
  eprint    = {1801.01290},
  timestamp = {Mon, 13 Aug 2018 16:48:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1801-01290.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{DeepRL,
    title={Playing Atari with Deep Reinforcement Learning},
    author={Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
    year={2013},
    eprint={1312.5602},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{DDQN,
    title={Deep Reinforcement Learning with Double {Q}-learning},
    author={Hado van Hasselt and Arthur Guez and David Silver},
    year={2015},
    eprint={1509.06461},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{lample2016playing,
    title={Playing FPS Games with Deep Reinforcement Learning},
    author={Guillaume Lample and Devendra Singh Chaplot},
    year={2016},
    eprint={1609.05521},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@article{BOTVINICK2019408,
title = "Reinforcement Learning, Fast and Slow",
journal = "Trends in Cognitive Sciences",
volume = "23",
number = "5",
pages = "408 - 422",
year = "2019",
issn = "1364-6613",
doi = "https://doi.org/10.1016/j.tics.2019.02.006",
url = "http://www.sciencedirect.com/science/article/pii/S1364661319300610",
author = "Matthew Botvinick and Sam Ritter and Jane X. Wang and Zeb Kurth-Nelson and Charles Blundell and Demis Hassabis",
abstract = "Deep reinforcement learning (RL) methods have driven impressive advances in artificial intelligence in recent years, exceeding human performance in domains ranging from Atari to Go to no-limit poker. This progress has drawn the attention of cognitive scientists interested in understanding human learning. However, the concern has been raised that deep RL may be too sample-inefficient – that is, it may simply be too slow – to provide a plausible model of how humans learn. In the present review, we counter this critique by describing recently developed techniques that allow deep RL to operate more nimbly, solving problems much more quickly than previous methods. Although these techniques were developed in an AI context, we propose that they may have rich implications for psychology and neuroscience. A key insight, arising from these AI methods, concerns the fundamental connection between fast RL and slower, more incremental forms of learning."
}

@article{mnist,
  added-at = {2010-06-28T21:16:30.000+0200},
  author = {LeCun, Yann and Cortes, Corinna},
  biburl = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/mhwombat},
  groups = {public},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  interhash = {21b9d0558bd66279df9452562df6e6f3},
  intrahash = {935bad99fa1f65e03c25b315aa3c1032},
  keywords = {MSc _checked character_recognition mnist network neural},
  lastchecked = {2016-01-14 14:24:11},
  timestamp = {2016-07-12T19:25:30.000+0200},
  title = {{MNIST} handwritten digit database},
  url = {http://yann.lecun.com/exdb/mnist/},
  username = {mhwombat},
  year = 2010
}

@misc{levine2018reinforcement,
      title={Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review}, 
      author={Sergey Levine},
      year={2018},
      eprint={1805.00909},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{SVI1,
  author    = {Wim Wiegerinck},
  editor    = {Craig Boutilier and
               Mois{\'{e}}s Goldszmidt},
  title     = {Variational Approximations between Mean Field Theory and the Junction
               Tree Algorithm},
  booktitle = {{UAI} '00: Proceedings of the 16th Conference in Uncertainty in Artificial
               Intelligence, Stanford University, Stanford, California, USA, June
               30 - July 3, 2000},
  pages     = {626--633},
  publisher = {Morgan Kaufmann},
  year      = {2000},
  url       = {https://dslpitt.org/uai/displayArticleDetails.jsp?mmnu=1\&smnu=2\&article\_id=73\&proceeding\_id=16},
  timestamp = {Wed, 06 May 2015 15:02:56 +0200},
  biburl    = {https://dblp.org/rec/conf/uai/Wiegerinck00.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{SVI2,
  author    = {Eric P. Xing and
               Michael I. Jordan and
               Stuart J. Russell},
  title     = {A Generalized Mean Field Algorithm for Variational Inference in Exponential
               Families},
  journal   = {CoRR},
  volume    = {abs/1212.2512},
  year      = {2012},
  url       = {http://arxiv.org/abs/1212.2512},
  archivePrefix = {arXiv},
  eprint    = {1212.2512},
  timestamp = {Mon, 13 Aug 2018 16:46:42 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1212-2512.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{910572,
  author={F. R. {Kschischang} and B. J. {Frey} and H. -. {Loeliger}},
  journal={IEEE Transactions on Information Theory}, 
  title={Factor graphs and the sum-product algorithm}, 
  year={2001},
  volume={47},
  number={2},
  pages={498-519}
}
  
@Article{Berridge2007,
author={Berridge, Kent C.},
title={The debate over dopamine's role in reward: the case for incentive salience},
journal={Psychopharmacology},
year={2007},
month={Apr},
day={01},
volume={191},
number={3},
pages={391-431},
abstract={Debate continues over the precise causal contribution made by mesolimbic dopamine systems to reward. There are three competing explanatory categories: `liking', learning, and `wanting'. Does dopamine mostly mediate the hedonic impact of reward (`liking')? Does it instead mediate learned predictions of future reward, prediction error teaching signals and stamp in associative links (learning)? Or does dopamine motivate the pursuit of rewards by attributing incentive salience to reward-related stimuli (`wanting')? Each hypothesis is evaluated here, and it is suggested that the incentive salience or `wanting' hypothesis of dopamine function may be consistent with more evidence than either learning or `liking'. In brief, recent evidence indicates that dopamine is neither necessary nor sufficient to mediate changes in hedonic `liking' for sensory pleasures. Other recent evidence indicates that dopamine is not needed for new learning, and not sufficient to directly mediate learning by causing teaching or prediction signals. By contrast, growing evidence indicates that dopamine does contribute causally to incentive salience. Dopamine appears necessary for normal `wanting', and dopamine activation can be sufficient to enhance cue-triggered incentive salience. Drugs of abuse that promote dopamine signals short circuit and sensitize dynamic mesolimbic mechanisms that evolved to attribute incentive salience to rewards. Such drugs interact with incentive salience integrations of Pavlovian associative information with physiological state signals. That interaction sets the stage to cause compulsive `wanting' in addiction, but also provides opportunities for experiments to disentangle `wanting', `liking', and learning hypotheses. Results from studies that exploited those opportunities are described here.},
issn={1432-2072},
doi={10.1007/s00213-006-0578-x},
url={https://doi.org/10.1007/s00213-006-0578-x}
}

@article {Schultz1593,
	author = {Schultz, Wolfram and Dayan, Peter and Montague, P. Read},
	title = {A Neural Substrate of Prediction and Reward},
	volume = {275},
	number = {5306},
	pages = {1593--1599},
	year = {1997},
	doi = {10.1126/science.275.5306.1593},
	publisher = {American Association for the Advancement of Science},
	abstract = {The capacity to predict future events permits a creature to detect, model, and manipulate the causal structure of its interactions with its environment. Behavioral experiments suggest that learning is driven by changes in the expectations about future salient events such as rewards and punishments. Physiological work has recently complemented these studies by identifying dopaminergic neurons in the primate whose fluctuating output apparently signals changes or errors in the predictions of future salient and rewarding events. Taken together, these findings can be understood through quantitative theories of adaptive optimizing control.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/275/5306/1593},
	eprint = {https://science.sciencemag.org/content/275/5306/1593.full.pdf},
	journal = {Science}
}

@misc{sophisticated,
      title={Sophisticated Inference}, 
      author={Karl Friston and Lancelot Da Costa and Danijar Hafner and Casper Hesp and Thomas Parr},
      year={2020},
      eprint={2006.04120},
      archivePrefix={arXiv},
      primaryClass={q-bio.NC}
}

@Article{BP_and_DC,
author={Yedidia, Jonathan S.},
title={Message-Passing Algorithms for Inference and Optimization},
journal={Journal of Statistical Physics},
year={2011},
month={Nov},
day={01},
volume={145},
number={4},
pages={860-890},
abstract={Message-passing algorithms can solve a wide variety of optimization, inference, and constraint satisfaction problems. The algorithms operate on factor graphs that visually represent and specify the structure of the problems. After describing some of their applications, I survey the family of belief propagation (BP) algorithms, beginning with a detailed description of the min-sum algorithm and its exactness on tree factor graphs, and then turning to a variety of more sophisticated BP algorithms, including free-energy based BP algorithms, ``splitting'' BP algorithms that generalize ``tree-reweighted'' BP, and the various BP algorithms that have been proposed to deal with problems with continuous variables.},
issn={1572-9613},
doi={10.1007/s10955-011-0384-7},
url={https://doi.org/10.1007/s10955-011-0384-7}
}

@Article{believe,
author = {Friston, Karl J. and Parr, Thomas and de Vries, Bert},
title = {The graphical brain: Belief propagation and active inference},
journal = {Network Neuroscience},
volume = {1},
number = {4},
pages = {381-414},
year = {2017},
doi = {10.1162/NETN\_a\_00018},
URL = {https://doi.org/10.1162/NETN_a_00018},
eprint = {https://doi.org/10.1162/NETN_a_00018},
abstract = { This paper considers functional integration in the brain from a computational perspective. We ask what sort of neuronal message passing is mandated by active inference—and what implications this has for context-sensitive connectivity at microscopic and macroscopic levels. In particular, we formulate neuronal processing as belief propagation under deep generative models. Crucially, these models can entertain both discrete and continuous states, leading to distinct schemes for belief updating that play out on the same (neuronal) architecture. Technically, we use Forney (normal) factor graphs to elucidate the requisite message passing in terms of its form and scheduling. To accommodate mixed generative models (of discrete and continuous states), one also has to consider link nodes or factors that enable discrete and continuous representations to talk to each other. When mapping the implicit computational architecture onto neuronal connectivity, several interesting features emerge. For example, Bayesian model averaging and comparison, which link discrete and continuous states, may be implemented in thalamocortical loops. These and other considerations speak to a computational connectome that is inherently state dependent and self-organizing in ways that yield to a principled (variational) account. We conclude with simulations of reading that illustrate the implicit neuronal message passing, with a special focus on how discrete (semantic) representations inform, and are informed by, continuous (visual) sampling of the sensorium.}
}

@Article{Friston2010,
author={Friston, Karl},
title={The free-energy principle: a unified brain theory?},
journal={Nature Reviews Neuroscience},
year={2010},
month={Feb},
day={01},
volume={11},
number={2},
pages={127-138},
abstract={Adaptive agents must occupy a limited repertoire of states and therefore minimize the long-term average of surprise associated with sensory exchanges with the world. Minimizing surprise enables them to resist a natural tendency to disorder.Surprise rests on predictions about sensations, which depend on an internal generative model of the world. Although surprise cannot be measured directly, a free-energy bound on surprise can be, suggesting that agents minimize free energy by changing their predictions (perception) or by changing the predicted sensory inputs (action).Perception optimizes predictions by minimizing free energy with respect to synaptic activity (perceptual inference), efficacy (learning and memory) and gain (attention and salience). This furnishes Bayes-optimal (probabilistic) representations of what caused sensations (providing a link to the Bayesian brain hypothesis).Bayes-optimal perception is mathematically equivalent to predictive coding and maximizing the mutual information between sensations and the representations of their causes. This is a probabilistic generalization of the principle of efficient coding (the infomax principle) or the minimum-redundancy principle.Learning under the free-energy principle can be formulated in terms of optimizing the connection strengths in hierarchical models of the sensorium. This rests on associative plasticity to encode causal regularities and appeals to the same synaptic mechanisms as those underlying cell assembly formation.Action under the free-energy principle reduces to suppressing sensory prediction errors that depend on predicted (expected or desired) movement trajectories. This provides a simple account of motor control, in which action is enslaved by perceptual (proprioceptive) predictions.Perceptual predictions rest on prior expectations about the trajectory or movement through the agent's state space. These priors can be acquired (as empirical priors during hierarchical inference) or they can be innate (epigenetic) and therefore subject to selective pressure.Predicted motion or state transitions realized by action correspond to policies in optimal control theory and reinforcement learning. In this context, value is inversely proportional to surprise (and implicitly free energy), and rewards correspond to innate priors that constrain policies.},
issn={1471-0048},
doi={10.1038/nrn2787},
url={https://doi.org/10.1038/nrn2787}
}

@article{BUCKLEY201755,
title = "The free energy principle for action and perception: A mathematical review",
journal = "Journal of Mathematical Psychology",
volume = "81",
pages = "55 - 79",
year = "2017",
issn = "0022-2496",
doi = "https://doi.org/10.1016/j.jmp.2017.09.004",
author = "Christopher L. Buckley and Chang Sub Kim and Simon McGregor and Anil K. Seth",
keywords = "Free energy principle, Perception, Action, Inference, Bayesian brain, Agent-based model",
}

@ARTICLE{active_vision,  author={D. {Ognibene} and G. {Baldassare}},  journal={IEEE Transactions on Autonomous Mental Development},   title={Ecological Active Vision: Four Bioinspired Principles to Integrate Bottom–Up and Adaptive Top–Down Attention Tested With a Simple Camera-Arm Robot},   year={2015},  volume={7},  number={1},  pages={3-25},}

@article{bellman1954,
author = "Bellman, Richard",
fjournal = "Bulletin of the American Mathematical Society",
journal = "Bull. Amer. Math. Soc.",
month = "11",
number = "6",
pages = "503--515",
publisher = "American Mathematical Society",
title = "The theory of dynamic programming",
url = "https://projecteuclid.org:443/euclid.bams/1183519147",
volume = "60",
year = "1954"
}

@article{10000025057,
author="Watkins, C. J. C. H.",
title="Learning from delayed rewards",
journal="PhD thesis, Cambridge University",
ISSN="",
publisher="",
year="1989",
month="",
volume="",
number="",
pages="",
URL="https://ci.nii.ac.jp/naid/10000025057/en/",
DOI="",
}

@incollection{NIPS2010_3964,
title = {Double {Q}-learning},
author = {Hado V. Hasselt},
booktitle = {Advances in Neural Information Processing Systems 23},
editor = {J. D. Lafferty and C. K. I. Williams and J. Shawe-Taylor and R. S. Zemel and A. Culotta},
pages = {2613--2621},
year = {2010},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/3964-double-q-learning.pdf}
}

@Article{adaptive_coding,
author={Duncan, John},
title={An adaptive coding model of neural function in prefrontal cortex},
journal={Nature Reviews Neuroscience},
year={2001},
month={Nov},
day={01},
volume={2},
number={11},
pages={820-829},
issn={1471-0048},
doi={10.1038/35097575},
url={https://doi.org/10.1038/35097575}
}

@inproceedings{Rish2001AnES,
  title={An empirical study of the naive {Bayes} classifier},
  author={Irina Rish},
  year={2001}
}

@inproceedings{Metsis06spamfiltering,
    author = {Vangelis Metsis and et al.},
    title = {Spam Filtering with Naive {Bayes} -- Which Naive {Bayes}?},
    booktitle = {THIRD CONFERENCE ON EMAIL AND ANTI-SPAM (CEAS},
    year = {2006},
    publisher = {}
}

@article{LDA,
author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
title = {Latent {Dirichlet} Allocation},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
month = mar,
pages = {993–1022},
numpages = {30}
}

@ARTICLE{HMM,
  author={L. {Rabiner} and B. {Juang}},
  journal={IEEE ASSP Magazine}, 
  title={An introduction to hidden {Markov} models}, 
  year={1986},
  volume={3},
  number={1},
  pages={4-16}
}

@Book{Sampling_Chapter,
author = { Christopher Bishop },
title = {Pattern Recognition and Machine Learning},
publisher = {Springer},
year = 2006,
pages = {738},
chapter = 11
}

@article{occam,
title = "Occam's Razor",
journal = "Information Processing Letters",
volume = "24",
number = "6",
pages = "377 - 380",
year = "1987",
issn = "0020-0190",
doi = "https://doi.org/10.1016/0020-0190(87)90114-1",
url = "http://www.sciencedirect.com/science/article/pii/0020019087901141",
author = "Anselm Blumer and Andrzej Ehrenfeucht and David Haussler and Manfred K. Warmuth",
keywords = "Machine learning, induction, inductive inference, Occam's Razor, methodology of science",
abstract = "We show that a polynomial learning algorithm, as defined by Valiant (1984), is obtained whenever there exists a polynomial-time method of producing, for any sequence of observations, a nearly minimum hypothesis that is consistent with these observations."
}

@inproceedings{KL_CONTROL,
author = {Rawlik, Konrad and Toussaint, Marc and Vijayakumar, Sethu},
title = {On Stochastic Optimal Control and Reinforcement Learning by Approximate Inference (Extended Abstract)},
year = {2013},
isbn = {9781577356332},
publisher = {AAAI Press},
booktitle = {Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence},
pages = {3052–3056},
numpages = {5},
location = {Beijing, China},
series = {IJCAI ’13}
}

@article{EPISTEMIC_VALUE,
author = {Karl Friston and Francesco Rigoli and Dimitri Ognibene and Christoph Mathys and Thomas Fitzgerald and Giovanni Pezzulo},
title = {Active inference and epistemic value},
journal = {Cognitive Neuroscience},
volume = {6},
number = {4},
pages = {187-214},
year  = {2015},
publisher = {Routledge},
doi = {10.1080/17588928.2015.1020053},
note ={PMID: 25689102},
URL = { https://doi.org/10.1080/17588928.2015.1020053},
eprint = { https://doi.org/10.1080/17588928.2015.1020053}
}

@Article{SC2,
author={Vinyals, Oriol
and Babuschkin, Igor
and Czarnecki, Wojciech M.
and Mathieu, Micha{\"e}l
and Dudzik, Andrew
and Chung, Junyoung
and Choi, David H.
and Powell, Richard
and Ewalds, Timo
and Georgiev, Petko
and Oh, Junhyuk
and Horgan, Dan
and Kroiss, Manuel
and Danihelka, Ivo
and Huang, Aja
and Sifre, Laurent
and Cai, Trevor
and Agapiou, John P.
and Jaderberg, Max
and Vezhnevets, Alexander S.
and Leblond, R{\'e}mi
and Pohlen, Tobias
and Dalibard, Valentin
and Budden, David
and Sulsky, Yury
and Molloy, James
and Paine, Tom L.
and Gulcehre, Caglar
and Wang, Ziyu
and Pfaff, Tobias
and Wu, Yuhuai
and Ring, Roman
and Yogatama, Dani
and W{\"u}nsch, Dario
and McKinney, Katrina
and Smith, Oliver
and Schaul, Tom
and Lillicrap, Timothy
and Kavukcuoglu, Koray
and Hassabis, Demis
and Apps, Chris
and Silver, David},
title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
journal={Nature},
year={2019},
month={Nov},
day={01},
volume={575},
number={7782},
pages={350-354},
abstract={Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1--3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8{\%} of officially ranked human players.},
issn={1476-4687},
doi={10.1038/s41586-019-1724-z},
url={https://doi.org/10.1038/s41586-019-1724-z}
}

@inproceedings{MAXQ,
  author    = {Thomas G. Dietterich},
  title     = {The {MAXQ} Method for Hierarchical Reinforcement Learning},
  booktitle = {Proceedings of the Fifteenth International Conference on Machine Learning
               {(ICML} 1998), Madison, Wisconsin, USA, July 24-27, 1998},
  pages     = {118--126},
  year      = {1998},
  crossref  = {DBLP:conf/icml/1998},
  timestamp = {Thu, 30 Jun 2011 10:34:12 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/Dietterich98.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@proceedings{DBLP:conf/icml/1998,
  editor    = {Jude W. Shavlik},
  title     = {Proceedings of the Fifteenth International Conference on Machine Learning
               {(ICML} 1998), Madison, Wisconsin, USA, July 24-27, 1998},
  publisher = {Morgan Kaufmann},
  year      = {1998},
  isbn      = {1-55860-556-8},
  timestamp = {Thu, 30 Jun 2011 10:34:12 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/1998.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{BHRL,
title = {{Bayesian} Hierarchical Reinforcement Learning},
author = {Feng Cao and Soumya Ray},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {73--81},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4752-bayesian-hierarchical-reinforcement-learning.pdf}
}

@article{FRISTON2018486,
title = "Deep temporal models and active inference",
journal = "Neuroscience \& Biobehavioral Reviews",
volume = "90",
pages = "486 - 501",
year = "2018",
issn = "0149-7634",
doi = "https://doi.org/10.1016/j.neubiorev.2018.04.004",
url = "http://www.sciencedirect.com/science/article/pii/S0149763418302525",
author = "Karl J. Friston and Richard Rosch and Thomas Parr and Cathy Price and Howard Bowman",
keywords = "Active inference, Bayesian, Hierarchical, Reading, Violation, Free energy, P300, MMN"
}

@article{Watkins1992,
	abstract = {
$$\mathcal\{Q\}$$  -learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem for     $$\mathcal\{Q\}$$  -learning based on that outlined in Watkins (1989). We show that     $$\mathcal\{Q\}$$  -learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where many     $$\mathcal\{Q\}$$   values can be changed each iteration, rather than just one.
},
	affiliation = {25b Framfield Road, N5 1UU Highbury, London, England; Centre for Cognitive Science, University of Edinburgh},
	author = {Watkins, Christopher J.C.H. and Dayan, Peter},
	copyright = {Kluwer Academic Publishers},
	doi = {10.1023/A:1022676722315},
	journal = {Machine Learning},
	keywords = {$$\mathcal\{Q\}$$  -learning; reinforcement learning; temporal differences; asynchronous dynamic programming},
	language = {English},
	number = {3-4},
	pages = {279-292},
	title = {Technical Note: Q-Learning},
	volume = {8},
	year = {1992},
}

@article{hippocampus_and_ghrelin,
author = {Kojima, Masayasu and Kangawa, Kenji},
title = {Ghrelin: Structure and Function},
journal = {Physiological Reviews},
volume = {85},
number = {2},
pages = {495-522},
year = {2005},
doi = {10.1152/physrev.00012.2004},
note ={PMID: 15788704},
URL = {https://doi.org/10.1152/physrev.00012.2004},
eprint = {https://doi.org/10.1152/physrev.00012.2004},
abstract = { Small synthetic molecules called growth hormone secretagogues (GHSs) stimulate the release of growth hormone (GH) from the pituitary. They act through the GHS-R, a G protein-coupled receptor whose ligand has only been discovered recently. Using a reverse pharmacology paradigm with a stable cell line expressing GHS-R, we purified an endogenous ligand for GHS-R from rat stomach and named it “ghrelin,” after a word root (“ghre”) in Proto-Indo-European languages meaning “grow.” Ghrelin is a peptide hormone in which the third amino acid, usually a serine but in some species a threonine, is modified by a fatty acid; this modification is essential for ghrelin's activity. The discovery of ghrelin indicates that the release of GH from the pituitary might be regulated not only by hypothalamic GH-releasing hormone, but also by ghrelin derived from the stomach. In addition, ghrelin stimulates appetite by acting on the hypothalamic arcuate nucleus, a region known to control food intake. Ghrelin is orexigenic; it is secreted from the stomach and circulates in the bloodstream under fasting conditions, indicating that it transmits a hunger signal from the periphery to the central nervous system. Taking into account all these activities, ghrelin plays important roles for maintaining GH release and energy homeostasis in vertebrates.}
}


@online{oleg,
  author = {Oleg Solopchuk},
  title = {Tutorial on Active Inference},
  year = 2018,
  url = {https://medium.com/@solopchuk/tutorial-on-active-inference-30edcf50f5dc},
  urldate = {2018-09-04}
}

@ARTICLE{BMR,
       author = {{Friston}, Karl and {Parr}, Thomas and {Zeidman}, Peter},
        title = "{{Bayesian} model reduction}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Methodology},
         year = 2018,
        month = may,
          eid = {arXiv:1805.07092},
        pages = {arXiv:1805.07092},
archivePrefix = {arXiv},
       eprint = {1805.07092},
 primaryClass = {stat.ME},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180507092F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article {BME,
	author = {Smith, Ryan and Schwartenbeck, Philipp and Parr, Thomas and Friston, Karl J.},
	title = {An active inference approach to modeling concept learning},
	elocation-id = {633677},
	year = {2019},
	doi = {10.1101/633677},
	publisher = {Cold Spring Harbor Laboratory},
	URL = {https://www.biorxiv.org/content/early/2019/05/23/633677},
	eprint = {https://www.biorxiv.org/content/early/2019/05/23/633677.full.pdf},
	journal = {bioRxiv}
}

@article{Pang_2019,
   title={On Reinforcement Learning for Full-Length Game of StarCraft},
   volume={33},
   ISSN={2159-5399},
   url={http://dx.doi.org/10.1609/aaai.v33i01.33014691},
   DOI={10.1609/aaai.v33i01.33014691},
   journal={Proceedings of the AAAI Conference on Artificial Intelligence},
   publisher={Association for the Advancement of Artificial Intelligence (AAAI)},
   author={Pang, Zhen-Jia and Liu, Ruo-Ze and Meng, Zhou-Yu and Zhang, Yi and Yu, Yang and Lu, Tong},
   year={2019},
   month={Jul},
   pages={4691–4698}
}

@misc{VAE,
    title={Tutorial on Variational Autoencoders},
    author={Carl Doersch},
    year={2016},
    eprint={1606.05908},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@inproceedings{beta-VAE,
  author    = {Irina Higgins and
               Lo{\"{\i}}c Matthey and
               Arka Pal and
               Christopher Burgess and
               Xavier Glorot and
               Matthew Botvinick and
               Shakir Mohamed and
               Alexander Lerchner},
  title     = {beta-{VAE}: Learning Basic Visual Concepts with a Constrained Variational
               Framework},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  year      = {2017},
  crossref  = {DBLP:conf/iclr/2017},
  url       = {https://openreview.net/forum?id=Sy2fzU9gl},
  timestamp = {Thu, 25 Jul 2019 14:25:41 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/HigginsMPBGBML17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@proceedings{DBLP:conf/iclr/2017,
  title     = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/group?id=ICLR.cc/2017/conference},
  timestamp = {Thu, 04 Apr 2019 13:20:07 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/2017.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{designpatterns,
    author = {Dietmar Kühl},
    title = {Design patterns for the implementation of graph algorithms},
    booktitle = {MASTER’S THESIS, TECHNISCHE UNIVERSITÄT},
    year = {1996},
    publisher = {}
}

@inproceedings{kar60614,
       booktitle = {Sixteenth International Conference on Autonomous Agents and Multiagent Sytems (AAMAS 2017)},
           month = {May},
           title = {Reward Shaping in Episodic Reinforcement Learning},
          author = {Marek Grzes},
       publisher = {ACM},
            year = {2017},
           pages = {565--573},
        keywords = {Reward structures for learning; Multiagent learning; Reward shaping; Reinforcement learning},
             url = {https://kar.kent.ac.uk/60614/},
        abstract = {Recent advancements in reinforcement learning confirm that reinforcement learning techniques can solve large scale problems leading to high quality autonomous decision making. It is a matter of time until we will see large scale applications of reinforcement learning in various sectors, such as healthcare and cyber-security, among others. However, reinforcement learning can be time-consuming because the learning algorithms have to determine the long term consequences of their actions using delayed feedback or rewards. Reward shaping is a method of incorporating domain knowledge into reinforcement learning so that the algorithms are guided faster towards more promising solutions. Under an overarching theme of episodic reinforcement learning, this paper shows a unifying analysis of potential-based reward shaping which leads to new theoretical insights into reward shaping in both model-free and model-based algorithms, as well as in multi-agent reinforcement learning.}
}

@article{10010902373,
author="NG, A. Y.",
title="Policy invariance under reward transformations: Theory and application to reward shaping",
journal="Proceedings of the 16th International Conference on Machine Learning",
ISSN="",
publisher="",
year="1999",
month="",
volume="",
number="",
pages="",
URL="https://ci.nii.ac.jp/naid/10010902373/en/",
DOI="",
}

############# Stuff related to CKA and representational similarity #####################################################
@article{Bonheme2022,
       author = {{Bonheme}, Lisa and {Grzes}, Marek},
        title = "{How do Variational Autoencoders Learn? Insights from Representational Similarity}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, I.2.6, G.3},
         year = 2022,
        month = may,
          eid = {arXiv:2205.08399},
        pages = {arXiv:2205.08399},
archivePrefix = {arXiv},
       eprint = {2205.08399},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2022arXiv220508399B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Cortes2012,
    author = {Cortes, Corinna and Mohri, Mehryar and Rostamizadeh, Afshin},
    title = {{Algorithms for Learning Kernels Based on Centered Alignment}},
    year = {2012},
    issue_date = {January 2012},
    publisher = {JMLR.org},
    volume = {13},
    number = {1},
    issn = {1532-4435},
    journal = {J. Mach. Learn. Res.},
    month = mar,
    pages = {795–828},
    numpages = {34},
    keywords = {learning kernels, kernel methods, feature selection}
}

@incollection{Cristianini2002,
    author = {Cristianini, Nello and Shawe-Taylor, John and Elisseeff, Andr{\'{e}} and Kandola, Jaz S},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {Dietterich, T G and Becker, S and Ghahramani, Z},
    pages = {367--373},
    volume = {14},
    publisher = {MIT Press},
    title = {{On Kernel-Target Alignment}},
    url = {http://papers.nips.cc/paper/1946-on-kernel-target-alignment.pdf},
    year = {2002},
    address = {Vancouver, Canada},
}

@inproceedings{Gretton2005,
    optaddress = {Berlin, Heidelberg},
    author = {Gretton, Arthur and Bousquet, Olivier and Smola, Alex and Sch{\"{o}}lkopf, Bernhard},
    booktitle = {Algorithmic Learning Theory},
    editor = {Jain, Sanjay and Simon, Hans Ulrich and Tomita, Etsuji},
    isbn = {978-3-540-31696-1},
    pages = {63--77},
    publisher = {Springer Berlin Heidelberg},
    title = {{Measuring Statistical Dependence with Hilbert-Schmidt Norms}},
    year = {2005}
}

@inproceedings{Kornblith2019,
    title = {{Similarity of Neural Network Representations Revisited}},
    author = {Simon Kornblith and Mohammad Norouzi and Honglak Lee and Geoffrey Hinton},
    booktitle = {Proceedings of the 36th International Conference on Machine Learning},
    pages = {3519--3529},
    year = {2019},
    editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
    volume = {97},
    series = {Proceedings of Machine Learning Research},
    address = {Long Beach, USA},
    month = {Jun},
    publisher = {PMLR},
    pdf = {http://proceedings.mlr.press/v97/kornblith19a/kornblith19a.pdf},
    url = {http://proceedings.mlr.press/v97/kornblith19a.html},
}

@inproceedings{Maheswaranathan2019,
 author = {Maheswaranathan, Niru and Williams, Alex and Golub, Matthew and Ganguli, Surya and Sussillo, David},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Universality and individuality in neural dynamics across large populations of recurrent networks},
 url = {https://proceedings.neurips.cc/paper/2019/file/5f5d472067f77b5c88f69f1bcfda1e08-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{Robert1976,
 ISSN = {00359254, 14679876},
 url = {http://www.jstor.org/stable/2347233},
 author = {P. Robert and Y. Escoufier},
 journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
 number = {3},
 pages = {257--265},
 publisher = {[Wiley, Royal Statistical Society]},
 title = {{A Unifying Tool for Linear Multivariate Statistical Methods: The RV- Coefficient}},
 volume = {25},
 year = {1976}
}