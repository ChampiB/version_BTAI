@article{MarkovBlanket,
author = {Kirchhoff, Michael  and Parr, Thomas  and Palacios, Ensor  and Friston, Karl  and Kiverstein, Julian },
title = {The Markov blankets of life: autonomy, active inference and the free energy principle},
journal = {Journal of The Royal Society Interface},
volume = {15},
number = {138},
pages = {20170792},
year = {2018},
doi = {10.1098/rsif.2017.0792},
URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rsif.2017.0792},
eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rsif.2017.0792},
    abstract = { This work addresses the autonomous organization of biological systems. It does so by considering the boundaries of biological systems, from individual cells to Home sapiens, in terms of the presence of Markov blankets under the active inference scheme—a corollary of the free energy principle. A Markov blanket defines the boundaries of a system in a statistical sense. Here we consider how a collective of Markov blankets can self-assemble into a global system that itself has a Markov blanket; thereby providing an illustration of how autonomous systems can be understood as having layers of nested and self-sustaining boundaries. This allows us to show that: (i) any living system is a Markov blanketed system and (ii) the boundaries of such systems need not be co-extensive with the biophysical boundaries of a living organism. In other words, autonomous systems are hierarchically composed of Markov blankets of Markov blankets—all the way down to individual cells, all the way up to you and me, and all the way out to include elements of the local environment. }
}

@article{POMDP_THESIS,
author="Sondik, E. J.",
title="The Optimal Control of Partially Observable Markov Processes",
journal="PhD thesis, Stanford University",
ISSN="",
publisher="",
year="1971",
month="",
volume="",
number="",
pages="",
URL="https://ci.nii.ac.jp/naid/20000916958/en/",
DOI="",
}

@ARTICLE{BAYESIAN_FILTERING,  author={Fox, V. and Hightower, J. and Lin Liao and Schulz, D. and Borriello, G.},  journal={IEEE Pervasive Computing},   title={Bayesian filtering for location estimation},   year={2003},  volume={2},  number={3},  pages={24-33},  doi={10.1109/MPRV.2003.1228524}
}

@article{bowman2007simultaneous,
  title={The Simultaneous Type, Serial Token Model of Temporal Attention and Working Memory},
  author={Bowman, Howard and Wyble, Brad},
  journal={Psychological Review},
  volume={114},
  number={1},
  pages={38--70},
  year={2007}
}

@article{wyble2006neural,
  title={A neural network account of binding discrete items into working memory using a distributed pool of flexible resources},
  author={Wyble, Brad and Bowman, Howard},
  journal={Journal of Vision},
  volume={6},
  number={6},
  pages={33--33a},
  year={2006},
  publisher={Association for Research in Vision and Ophthalmology}
}

@article{rafetseder2013counterfactual,
  title={Counterfactual reasoning: From childhood to adulthood},
  author={Rafetseder, Eva and Schwitalla, Maria and Perner, Josef},
  journal={Journal of experimental child psychology},
  volume={114},
  number={3},
  pages={389--404},
  year={2013},
  publisher={Elsevier}
}

@article{EI_for_BO,
  title={A Tutorial on {Bayesian} Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning},
  author={Brochu, Eric and Cora, Vlad M and de Freitas, Nando},
  journal={arXiv preprint arXiv:1012.2599},
  year={2010},
  publisher={Citeseer}
}

@inproceedings{ExpectedInprovement,
 author = {Bergstra, James and Bardenet, R\'{e}mi and Bengio, Yoshua and K\'{e}gl, Bal\'{a}zs},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Algorithms for Hyper-Parameter Optimization},
 url = {https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf},
 volume = {24},
 year = {2011}
}

@inproceedings{Thompson_and_MCTS,
 address = {Lake Tahoe, United States},
 author = {Aijun Bai and Feng Wu and Xiaoping Chen},
 booktitle = {Proceedings of the Advances in Neural Information Processing Systems (NIPS)},
 month = {December},
 pages = {1646-1654},
 title = {Bayesian Mixture Modelling and Inference based {Thompson} Sampling in {Monte-Carlo} Tree Search},
 year = {2013}
}

@article{AI_and_Thompson,
    author = {Sajid, Noor and Ball, Philip J. and Parr, Thomas and Friston, Karl J.},
    title = "{Active Inference: Demystified and Compared}",
    journal = {Neural Computation},
    volume = {33},
    number = {3},
    pages = {674-712},
    year = {2021},
    month = {03},
    abstract = "{Active inference is a first principle account of how autonomous agents operate in dynamic, nonstationary environments. This problem is also considered in reinforcement learning, but limited work exists on comparing the two approaches on the same discrete-state environments. In this letter, we provide (1) an accessible overview of the discrete-state formulation of active inference, highlighting natural behaviors in active inference that are generally engineered in reinforcement learning, and (2) an explicit discrete-state comparison between active inference and reinforcement learning on an OpenAI gym baseline. We begin by providing a condensed overview of the active inference literature, in particular viewing the various natural behaviors of active inference agents through the lens of reinforcement learning. We show that by operating in a pure belief-based setting, active inference agents can carry out epistemic exploration—and account for uncertainty about their environment—in a Bayes-optimal fashion. Furthermore, we show that the reliance on an explicit reward signal in reinforcement learning is removed in active inference, where reward can simply be treated as another observation we have a preference over; even in the total absence of rewards, agent behaviors are learned through preference learning. We make these properties explicit by showing two scenarios in which active inference agents can infer behaviors in reward-free environments compared to both Q-learning and Bayesian model-based reinforcement learning agents and by placing zero prior preferences over rewards and learning the prior preferences over the observations corresponding to reward. We conclude by noting that this formalism can be applied to more complex settings (e.g., robotic arm movement, Atari games) if appropriate generative models can be formulated. In short, we aim to demystify the behavior of active inference agents by presenting an accessible discrete state-space and time formulation and demonstrate these behaviors in a OpenAI gym environment, alongside reinforcement learning agents.}",
    issn = {0899-7667},
    doi = {10.1162/neco_a_01357},
    url = {https://doi.org/10.1162/neco\_a\_01357},
    eprint = {https://direct.mit.edu/neco/article-pdf/33/3/674/1889396/neco\_a\_01357.pdf},
}

@article{TutoThompson,
author = {Russo, Daniel J. and Van Roy, Benjamin and Kazerouni, Abbas and Osband, Ian and Wen, Zheng},
title = {A Tutorial on {Thompson} Sampling},
year = {2018},
issue_date = {12 7 2018},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {11},
number = {1},
issn = {1935-8237},
url = {https://doi.org/10.1561/2200000070},
doi = {10.1561/2200000070},
abstract = {Thompson sampling is an algorithm for online decision problemswhere actions are taken sequentially in a manner thatmust balance between exploiting what is known to maximizeimmediate performance and investing to accumulatenew information that may improve future performance. Thealgorithm addresses a broad range of problems in a computationallyefficient manner and is therefore enjoying wideuse. This tutorial covers the algorithm and its application,illustrating concepts through a range of examples, includingBernoulli bandit problems, shortest path problems, productrecommendation, assortment, active learning with neuralnetworks, and reinforcement learning in Markov decisionprocesses. Most of these problems involve complex informationstructures, where information revealed by taking anaction informs beliefs about other actions. We will also discusswhen and why Thompson sampling is or is not effectiveand relations to alternative algorithms.},
journal = {Found. Trends Mach. Learn.},
month = {jul},
pages = {1–96},
numpages = {96},
keywords = {Exploration, Bandit learning}
}

@article{ThompsonOriginal,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2332286},
 author = {William R. Thompson},
 journal = {Biometrika},
 number = {3/4},
 pages = {285--294},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {On the Likelihood that One Unknown Probability Exceeds Another in View of the Evidence of Two Samples},
 volume = {25},
 year = {1933}
}
    
@article{LargePOMDP,
  author    = {Domenico Maisto and
               Francesco Gregoretti and
               Karl J. Friston and
               Giovanni Pezzulo},
  title     = {Active Tree Search in Large {POMDPs}},
  journal   = {CoRR},
  volume    = {abs/2103.13860},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.13860},
  eprinttype = {arXiv},
  eprint    = {2103.13860},
  timestamp = {Wed, 07 Apr 2021 15:31:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-13860.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Sophisticated_INF,
    author = {Friston, Karl and Da Costa, Lancelot and Hafner, Danijar and Hesp, Casper and Parr, Thomas},
    title = "{Sophisticated Inference}",
    journal = {Neural Computation},
    volume = {33},
    number = {3},
    pages = {713-763},
    year = {2021},
    month = {03},
    abstract = "{Active inference offers a first principle account of sentient behavior, from which special and important cases—for example, reinforcement learning, active learning, Bayes optimal inference, Bayes optimal design—can be derived. Active inference finesses the exploitation-exploration dilemma in relation to prior preferences by placing information gain on the same footing as reward or value. In brief, active inference replaces value functions with functionals of (Bayesian) beliefs, in the form of an expected (variational) free energy. In this letter, we consider a sophisticated kind of active inference using a recursive form of expected free energy. Sophistication describes the degree to which an agent has beliefs about beliefs. We consider agents with beliefs about the counterfactual consequences of action for states of affairs and beliefs about those latent states. In other words, we move from simply considering beliefs about “what would happen if I did that” to “what I would believe about what would happen if I did that.” The recursive form of the free energy functional effectively implements a deep tree search over actions and outcomes in the future. Crucially, this search is over sequences of belief states as opposed to states per se. We illustrate the competence of this scheme using numerical simulations of deep decision problems.}",
    issn = {0899-7667},
    doi = {10.1162/neco_a_01351},
    url = {https://doi.org/10.1162/neco\_a\_01351},
    eprint = {https://direct.mit.edu/neco/article-pdf/33/3/713/1889421/neco\_a\_01351.pdf},
}

@inproceedings{DBLP:conf/ecml/KocsisS06,
  author    = {Levente Kocsis and
               Csaba Szepesv{\'{a}}ri},
  editor    = {Johannes F{\"{u}}rnkranz and
               Tobias Scheffer and
               Myra Spiliopoulou},
  title     = {Bandit Based {Monte-Carlo} Planning},
  booktitle = {Machine Learning: {ECML} 2006, 17th European Conference on Machine
               Learning, Berlin, Germany, September 18-22, 2006, Proceedings},
  series    = {Lecture Notes in Computer Science},
  volume    = {4212},
  pages     = {282--293},
  publisher = {Springer},
  year      = {2006},
  url       = {https://doi.org/10.1007/11871842\_29},
  doi       = {10.1007/11871842\_29},
  timestamp = {Tue, 14 May 2019 10:00:54 +0200},
  biburl    = {https://dblp.org/rec/conf/ecml/KocsisS06.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{hafner2020action,
  author    = {Danijar Hafner and
               Pedro A. Ortega and
               Jimmy Ba and
               Thomas Parr and
               Karl J. Friston and
               Nicolas Heess},
  title     = {Action and Perception as Divergence Minimization},
  journal   = {CoRR},
  volume    = {abs/2009.01791},
  year      = {2020},
  url       = {https://arxiv.org/abs/2009.01791},
  archivePrefix = {arXiv},
  eprint    = {2009.01791},
  timestamp = {Wed, 16 Sep 2020 15:27:56 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2009-01791.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{concurrency_glabbeek,
author = {Glabbeek, Rob J. van},
title = {The Linear Time-Branching Time Spectrum (Extended Abstract)},
year = {1990},
isbn = {3540530487},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Proceedings of the Theories of Concurrency: Unification and Extension},
pages = {278–297},
numpages = {20},
series = {CONCUR '90}
}

@InProceedings{concurrency_glabbeek_2,
author="van Glabbeek, R. J.",
editor="Best, Eike",
title="The linear time --- Branching time spectrum {II}",
booktitle="CONCUR'93",
year="1993",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="66--81",
abstract="This paper studies semantic equivalences and preorders for sequential systems with silent moves, restricting attention to the ones that abstract from successful termination, stochastic and real-time aspects of the investigated systems, and the structure of the visible actions systems can perform. It provides a parameterized definition of such a preorder, such that most such preorders and equivalences found in the literature are obtained by a suitable instantiation of the parameters. Other instantiations yield preorders that combine properties from various semantics. Moreover, the approach shows several ways in which preorders that were originally only considered for systems without silent moves, most notably the ready simulation, can be generalized to an abstract setting, and how preorders that were originally only considered for for systems without divergence, such as the coupled simulation, can be extended to divergent systems. All preorders come with---or rather as---a modal characterization, and when possible also a relational characterization. The paper concludes with some pros and cons of the preorders.",
isbn="978-3-540-47968-0"
}

@inproceedings{DeepAIwithMCMC,
  author    = {Zafeirios Fountas and
               Noor Sajid and
               Pedro A. M. Mediano and
               Karl J. Friston},
  editor    = {Hugo Larochelle and
               Marc'Aurelio Ranzato and
               Raia Hadsell and
               Maria{-}Florina Balcan and
               Hsuan{-}Tien Lin},
  title     = {Deep active inference agents using {Monte-Carlo} methods},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
               on Neural Information Processing Systems 2020, NeurIPS 2020, December
               6-12, 2020, virtual},
  year      = {2020},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/865dfbde8a344b44095495f3591f7407-Abstract.html},
  timestamp = {Tue, 19 Jan 2021 15:56:58 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/FountasSMF20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{AI_TUTO,
title = {Active inference on discrete state-spaces: A synthesis},
journal = {Journal of Mathematical Psychology},
volume = {99},
pages = {102447},
year = {2020},
issn = {0022-2496},
doi = {https://doi.org/10.1016/j.jmp.2020.102447},
url = {https://www.sciencedirect.com/science/article/pii/S0022249620300857},
author = {Lancelot {Da Costa} and Thomas Parr and Noor Sajid and Sebastijan Veselic and Victorita Neacsu and Karl Friston},
keywords = {Active inference, Free energy principle, Process theory, Variational Bayesian inference, Markov decision process, Mathematical review},
abstract = {Active inference is a normative principle underwriting perception, action, planning, decision-making and learning in biological or artificial agents. From its inception, its associated process theory has grown to incorporate complex generative models, enabling simulation of a wide range of complex behaviours. Due to successive developments in active inference, it is often difficult to see how its underlying principle relates to process theories and practical implementation. In this paper, we try to bridge this gap by providing a complete mathematical synthesis of active inference on discrete state-space models. This technical summary provides an overview of the theory, derives neuronal dynamics from first principles and relates this dynamics to biological processes. Furthermore, this paper provides a fundamental building block needed to understand active inference for mixed generative models; allowing continuous sensations to inform discrete representations. This paper may be used as follows: to guide research towards outstanding challenges, a practical guide on how to implement active inference to simulate experimental behaviour, or a pointer towards various in-silico neurophysiological responses that may be used to make empirical predictions.}
}

@article{AI_VMP,
    author = {Champion, Théophile and Grześ, Marek and Bowman, Howard},
    title = "{Realizing Active Inference in Variational Message Passing: The Outcome-Blind Certainty Seeker}",
    journal = {Neural Computation},
    pages = {1-65},
    year = {2021},
    month = {07},
    abstract = "{Active inference is a state-of-the-art framework in neuroscience that offers a unified theory of brain function. It is also proposed as a framework for planning in AI. Unfortunately, the complex mathematics required to create new models can impede application of active inference in neuroscience and AI research. This letter addresses this problem by providing a complete mathematical treatment of the active inference framework in discrete time and state spaces and the derivation of the update equations for any new model. We leverage the theoretical connection between active inference and variational message passing as described by John Winn and Christopher M. Bishop in 2005. Since variational message passing is a well-defined methodology for deriving Bayesian belief update equations, this letter opens the door to advanced generative models for active inference. We show that using a fully factorized variational distribution simplifies the expected free energy, which furnishes priors over policies so that agents seek unambiguous states. Finally, we consider future extensions that support deep tree searches for sequential policy optimization based on structure learning and belief propagation.}",
    issn = {0899-7667},
    doi = {10.1162/neco_a_01422},
    url = {https://doi.org/10.1162/neco\_a\_01422},
    eprint = {https://direct.mit.edu/neco/article-pdf/doi/10.1162/neco\_a\_01422/1930278/neco\_a\_01422.pdf},
}

@article{UBC1_nln_opt,
author = {Lai, T.L and Robbins, Herbert},
title = {Asymptotically Efficient Adaptive Allocation Rules},
year = {1985},
issue_date = {March, 1985},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {6},
number = {1},
issn = {0196-8858},
url = {https://doi.org/10.1016/0196-8858(85)90002-8},
doi = {10.1016/0196-8858(85)90002-8},
journal = {Adv. Appl. Math.},
month = mar,
pages = {4–22},
numpages = {19}
}

@InProceedings{10.1007/11871842_29,
author="Kocsis, Levente
and Szepesv{\'a}ri, Csaba",
editor="F{\"u}rnkranz, Johannes
and Scheffer, Tobias
and Spiliopoulou, Myra",
title="Bandit Based {Monte-Carlo} Planning",
booktitle="Machine Learning: ECML 2006",
year="2006",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="282--293",
abstract="For large state-space Markovian Decision Problems Monte-Carlo planning is one of the few viable approaches to find near-optimal solutions. In this paper we introduce a new algorithm, UCT, that applies bandit ideas to guide Monte-Carlo planning. In finite-horizon or discounted MDPs the algorithm is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling. Experimental results show that in several domains, UCT is significantly more efficient than its alternatives.",
isbn="978-3-540-46056-5"
}

@article{DBLP:journals/neco/OReilly96,
  author    = {Randall C. O'Reilly},
  title     = {Biologically Plausible Error-Driven Learning Using Local Activation
               Differences: The Generalized Recirculation Algorithm},
  journal   = {Neural Comput.},
  volume    = {8},
  number    = {5},
  pages     = {895--938},
  year      = {1996},
  url       = {https://doi.org/10.1162/neco.1996.8.5.895},
  doi       = {10.1162/neco.1996.8.5.895},
  timestamp = {Tue, 01 Sep 2020 13:11:27 +0200},
  biburl    = {https://dblp.org/rec/journals/neco/OReilly96.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{PUCB,
  author    = {Christopher D. Rosin},
  title     = {Multi-armed bandits with episode context},
  booktitle = {International Symposium on Artificial Intelligence and Mathematics,
               {ISAIM} 2010, Fort Lauderdale, Florida, USA, January 6-8, 2010},
  year      = {2010},
  url       = {http://gauss.ececs.uc.edu/Workshops/isaim2010/papers/rosin.pdf},
  timestamp = {Thu, 12 Mar 2020 11:39:29 +0100},
  biburl    = {https://dblp.org/rec/conf/isaim/Rosin10.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Article{Auer2002,
author={Auer, Peter and Cesa-Bianchi, Nicol{\`o} and Fischer, Paul},
title={Finite-time Analysis of the Multiarmed Bandit Problem},
journal={Machine Learning},
year={2002},
month={May},
day={01},
volume={47},
number={2},
pages={235-256},
abstract={Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
issn={1573-0565},
doi={10.1023/A:1013689704352},
url={https://doi.org/10.1023/A:1013689704352}
}

@book{concurrency_howard,
      author        = "Bowman, Howard",
      title         = "{Concurrency Theory: Calculi an Automata for Modelling
                       Untimed and Timed Concurrent Systems}",
      publisher     = "Springer",
      address       = "Dordrecht",
      year          = "2005",
      url           = "https://cds.cern.ch/record/1250124",
}

@Article{Parr2019,
author={Parr, Thomas
and Friston, Karl J.},
title={Generalised free energy and active inference},
journal={Biological Cybernetics},
year={2019},
month={Dec},
day={01},
volume={113},
number={5},
pages={495-513},
abstract={Active inference is an approach to understanding behaviour that rests upon the idea that the brain uses an internal generative model to predict incoming sensory data. The fit between this model and data may be improved in two ways. The brain could optimise probabilistic beliefs about the variables in the generative model (i.e. perceptual inference). Alternatively, by acting on the world, it could change the sensory data, such that they are more consistent with the model. This implies a common objective function (variational free energy) for action and perception that scores the fit between an internal model and the world. We compare two free energy functionals for active inference in the framework of Markov decision processes. One of these is a functional of beliefs (i.e. probability distributions) about states and policies, but a function of observations, while the second is a functional of beliefs about all three. In the former (expected free energy), prior beliefs about outcomes are not part of the generative model (because they are absorbed into the prior over policies). Conversely, in the second (generalised free energy), priors over outcomes become an explicit component of the generative model. When using the free energy function, which is blind to future observations, we equip the generative model with a prior over policies that ensure preferred (i.e. priors over) outcomes are realised. In other words, if we expect to encounter a particular kind of outcome, this lends plausibility to those policies for which this outcome is a consequence. In addition, this formulation ensures that selected policies minimise uncertainty about future outcomes by minimising the free energy expected in the future. When using the free energy functional---that effectively treats future observations as hidden states---we show that policies are inferred or selected that realise prior preferences by minimising the free energy of future expectations. Interestingly, the form of posterior beliefs about policies (and associated belief updating) turns out to be identical under both formulations, but the quantities used to compute them are not.},
issn={1432-0770},
doi={10.1007/s00422-019-00805-w},
url={https://doi.org/10.1007/s00422-019-00805-w}
}

@article{Go,
  author    = {David Silver and
               Aja Huang and
               Chris J. Maddison and
               Arthur Guez and
               Laurent Sifre and
               George van den Driessche and
               Julian Schrittwieser and
               Ioannis Antonoglou and
               Vedavyas Panneershelvam and
               Marc Lanctot and
               Sander Dieleman and
               Dominik Grewe and
               John Nham and
               Nal Kalchbrenner and
               Ilya Sutskever and
               Timothy P. Lillicrap and
               Madeleine Leach and
               Koray Kavukcuoglu and
               Thore Graepel and
               Demis Hassabis},
  title     = {{Mastering the game of Go with deep neural networks and tree search}},
  journal   = {Nature},
  volume    = {529},
  number    = {7587},
  pages     = {484--489},
  year      = {2016},
  url       = {https://doi.org/10.1038/nature16961},
  doi       = {10.1038/nature16961},
  timestamp = {Wed, 14 Nov 2018 10:30:42 +0100},
  biburl    = {https://dblp.org/rec/journals/nature/SilverHMGSDSAPL16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{FRISTON2016862,
	title = "Active inference and learning",
	journal = "Neuroscience \& Biobehavioral Reviews",
	volume = "68",
	pages = "862 - 879",
	year = "2016",
	issn = "0149-7634",
	doi = "https://doi.org/10.1016/j.neubiorev.2016.06.022",
	author = "Karl Friston and Thomas FitzGerald and Francesco Rigoli and Philipp Schwartenbeck and John O Doherty and Giovanni Pezzulo",
	keywords = "Active inference, Habit learning, Bayesian inference, Goal-directed, Free energy, Information gain, Bayesian surprise, Epistemic value, Exploration, Exploitation",
}

@article{bayes_surprise,
title = "{Bayesian} surprise attracts human attention",
journal = "Vision Research",
volume = "49",
number = "10",
pages = "1295 - 1306",
year = "2009",
note = "Visual Attention: Psychophysics, electrophysiology and neuroimaging",
issn = "0042-6989",
doi = "https://doi.org/10.1016/j.visres.2008.09.007",
url = "http://www.sciencedirect.com/science/article/pii/S0042698908004380",
author = "Laurent Itti and Pierre Baldi",
keywords = "Attention, Surprise, Bayes theorem, Information theory, Eye movements, Natural vision, Free viewing, Saliency, Novelty",
abstract = "We propose a formal Bayesian definition of surprise to capture subjective aspects of sensory information. Surprise measures how data affects an observer, in terms of differences between posterior and prior beliefs about the world. Only data observations which substantially affect the observer’s beliefs yield surprise, irrespectively of how rare or informative in Shannon’s sense these observations are. We test the framework by quantifying the extent to which humans may orient attention and gaze towards surprising events or items while watching television. To this end, we implement a simple computational model where a low-level, sensory form of surprise is computed by simple simulated early visual neurons. Bayesian surprise is a strong attractor of human attention, with 72% of all gaze shifts directed towards locations more surprising than the average, a figure rising to 84% when focusing the analysis onto regions simultaneously selected by all observers. The proposed theory of surprise is applicable across different spatio-temporal scales, modalities, and levels of abstraction."
}

@article {curiosity,
	author = {Schwartenbeck, Philipp and Passecker, Johannes and Hauser, Tobias U and FitzGerald, Thomas H B and Kronbichler, Martin and Friston, Karl},
	title = {Computational mechanisms of curiosity and goal-directed exploration},
	elocation-id = {411272},
	year = {2018},
	doi = {10.1101/411272},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Successful behaviour depends on the right balance between maximising reward and soliciting information about the world. Here, we show how different types of information-gain emerge when casting behaviour as surprise minimisation. We present two distinct mechanisms for goal-directed exploration that express separable profiles of active sampling to reduce uncertainty. {\textquoteleft}Hidden state{\textquoteright} exploration motivates agents to sample unambiguous observations to accurately infer the (hidden) state of the world. Conversely, {\textquoteleft}model parameter{\textquoteright} exploration, compels agents to sample outcomes associated with high uncertainty, if they are informative for their representation of the task structure. We illustrate the emergence of these types of information-gain, termed active inference and active learning, and show how these forms of exploration induce distinct patterns of {\textquoteleft}Bayes-optimal{\textquoteright} behaviour. Our findings provide a computational framework to understand how distinct levels of uncertainty induce different modes of information-gain in decision-making.},
	URL = {https://www.biorxiv.org/content/early/2018/09/07/411272},
	eprint = {https://www.biorxiv.org/content/early/2018/09/07/411272.full.pdf},
	journal = {bioRxiv}
}

@ARTICLE{dopamine,
AUTHOR={FitzGerald, Thomas H. B. and Dolan, Raymond J. and Friston, Karl},
TITLE={Dopamine, reward learning, and active inference},      
JOURNAL={Frontiers in Computational Neuroscience},
VOLUME={9},
PAGES={136},
YEAR={2015},
URL={https://www.frontiersin.org/article/10.3389/fncom.2015.00136},
DOI={10.3389/fncom.2015.00136},
ISSN={1662-5188},   
ABSTRACT={Temporal difference learning models propose phasic dopamine signaling encodes reward prediction errors that drive learning. This is supported by studies where optogenetic stimulation of dopamine neurons can stand in lieu of actual reward. Nevertheless, a large body of data also shows that dopamine is not necessary for learning, and that dopamine depletion primarily affects task performance. We offer a resolution to this paradox based on an hypothesis that dopamine encodes the precision of beliefs about alternative actions, and thus controls the outcome-sensitivity of behavior. We extend an active inference scheme for solving Markov decision processes to include learning, and show that simulated dopamine dynamics strongly resemble those actually observed during instrumental conditioning. Furthermore, simulated dopamine depletion impairs performance but spares learning, while simulated excitation of dopamine neurons drives reward learning, through aberrant inference about outcome states. Our formal approach provides a novel and parsimonious reconciliation of apparently divergent experimental findings.}
}

@article{PAI,
title = "Planning as inference",
journal = "Trends in Cognitive Sciences",
volume = "16",
number = "10",
pages = "485 - 488",
year = "2012",
issn = "1364-6613",
doi = "https://doi.org/10.1016/j.tics.2012.08.006",
author = "Matthew Botvinick and Marc Toussaint",
}

@article{pezzato2020active,
  author    = {Corrado Pezzato and
               Carlos Hern{\'{a}}ndez Corbato and
               Martijn Wisse},
  title     = {Active Inference and Behavior Trees for Reactive Action Planning and
               Execution in Robotics},
  journal   = {CoRR},
  volume    = {abs/2011.09756},
  year      = {2020},
  url       = {https://arxiv.org/abs/2011.09756},
  archivePrefix = {arXiv},
  eprint    = {2011.09756},
  timestamp = {Wed, 25 Nov 2020 16:34:14 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2011-09756.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{catal2020learning,
  author    = {Ozan {\c{C}}atal and
               Tim Verbelen and
               Johannes Nauta and
               Cedric De Boom and
               Bart Dhoedt},
  title     = {Learning Perception and Planning With Deep Active Inference},
  booktitle = {2020 {IEEE} International Conference on Acoustics, Speech and Signal
               Processing, {ICASSP} 2020, Barcelona, Spain, May 4-8, 2020},
  pages     = {3952--3956},
  publisher = {{IEEE}},
  year      = {2020},
  url       = {https://doi.org/10.1109/ICASSP40776.2020.9054364},
  doi       = {10.1109/ICASSP40776.2020.9054364},
  timestamp = {Thu, 23 Jul 2020 16:20:10 +0200},
  biburl    = {https://dblp.org/rec/conf/icassp/CatalVNBD20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{sancaktar2020endtoend,
  author    = {Cansu Sancaktar and
               Marcel A. J. van Gerven and
               Pablo Lanillos},
  title     = {End-to-End Pixel-Based Deep Active Inference for Body Perception and
               Action},
  booktitle = {Joint {IEEE} 10th International Conference on Development and Learning
               and Epigenetic Robotics, ICDL-EpiRob 2020, Valparaiso, Chile, October
               26-30, 2020},
  pages     = {1--8},
  publisher = {{IEEE}},
  year      = {2020},
  url       = {https://doi.org/10.1109/ICDL-EpiRob48136.2020.9278105},
  doi       = {10.1109/ICDL-EpiRob48136.2020.9278105},
  timestamp = {Mon, 08 Feb 2021 13:53:48 +0100},
  biburl    = {https://dblp.org/rec/conf/icdl-epirob/SancaktarGL20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{CULLEN2018809,
title = "Active Inference in OpenAI Gym: A Paradigm for Computational Investigations Into Psychiatric Illness",
journal = "Biological Psychiatry: Cognitive Neuroscience and Neuroimaging",
volume = "3",
number = "9",
pages = "809 - 818",
year = "2018",
note = "Computational Methods and Modeling in Psychiatry",
issn = "2451-9022",
doi = "https://doi.org/10.1016/j.bpsc.2018.06.010",
url = "http://www.sciencedirect.com/science/article/pii/S2451902218301617",
author = "Maell Cullen and Ben Davey and Karl J. Friston and Rosalyn J. Moran",
keywords = "Active inference, Computational phenotyping, Computational psychiatry, Free energy principle, Game-based imaging biomarkers, Markov decision process",
abstract = "Background
Artificial intelligence has recently attained humanlike performance in a number of gamelike domains. These advances have been spurred by brain-inspired architectures and algorithms such as hierarchical filtering and reinforcement learning. OpenAI Gym is an open-source platform in which to train, test, and benchmark algorithms—it provides a range of tasks, including those of classic arcade games such as Doom. Here we describe how the platform might be used as a simulation, test, and diagnostic paradigm for psychiatric conditions.
Methods
To illustrate how active inference models of game play could be used to test mechanistic and algorithmic properties of psychiatric disorders, we provide two exemplar analyses. The first speaks to the impact of aging on cognition, examining game-play behaviors in a model of aging in which we compared age-dependent changes of younger (n = 9, 22 ± 1 years of age) and older (n = 7, 56 ± 5 years of age) adult players. The second is an illustration of a putative feature of anhedonia in which we simulated diminished sensitivity to reward.
Results
These simulations demonstrate how active inference can be used to test predicted changes in both neurobiology and beliefs in psychiatric cohorts. We show that, as well as behavioral measures, putative neural correlates of active inference can be simulated, and hypothesized (model-based) differences in local field potentials and blood oxygen level–dependent responses can be produced.
Conclusions
We show that active inference, through epistemic and value-based goals, enables simulated subjects to actively develop detailed representations of gaming environments, and we demonstrate the use of a principled algorithmic and neurobiological framework for testing hypotheses in psychiatric illness."
}

@misc{cart_pole,
      author={Beren Millidge},
      title={Combining Active Inference and Hierarchical Predictive Coding: A Tutorial Introduction and Case Study.}, 
      year={2019},
      url = {https://doi.org/10.31234/osf.io/kf6wc}
}

@Article{VI_TUTO,
author={Fox, Charles W.
and Roberts, Stephen J.},
title={A tutorial on variational {Bayesian} inference},
journal={Artificial Intelligence Review},
year={2012},
month={Aug},
day={01},
volume={38},
number={2},
pages={85-95},
abstract={This tutorial describes the mean-field variational Bayesian approximation to inference in graphical models, using modern machine learning terminology rather than statistical physics concepts. It begins by seeking to find an approximate mean-field distribution close to the target joint in the KL-divergence sense. It then derives local node updates and reviews the recent Variational Message Passing framework.},
issn={1573-7462},
doi={10.1007/s10462-011-9236-8},
url={https://doi.org/10.1007/s10462-011-9236-8}
}

@Article{VMP_TUTO,
author = { John Winn and Christopher Bishop },
title = {Variational Message Passing},
journal = {Journal of Machine Learning Research},
publisher = {Microtome publishing},
year = 2005,
volume = {6},
pages = {661-694}
}

@ARTICLE{FFG_TUTO,
  author={G. D. {Forney}},
  journal={IEEE Transactions on Information Theory}, 
  title={Codes on graphs: normal realizations}, 
  year={2001},
  volume={47},
  number={2},
  pages={520-548}
}

@article{Simul_AI,
  author    = {Thijs van de Laar and
               Bert de Vries},
  title     = {Simulating Active Inference Processes by Message Passing},
  journal   = {Front. Robotics and {AI}},
  volume    = {2019},
  year      = {2019},
  url       = {https://doi.org/10.3389/frobt.2019.00020},
  doi       = {10.3389/frobt.2019.00020},
  timestamp = {Tue, 16 Apr 2019 16:32:33 +0200},
  biburl    = {https://dblp.org/rec/journals/firai/LaarV19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/ijar/CoxLV19,
  author    = {Marco Cox and
               Thijs van de Laar and
               Bert de Vries},
  title     = {A factor graph approach to automated design of {Bayesian} signal processing
               algorithms},
  journal   = {Int. J. Approx. Reason.},
  volume    = {104},
  pages     = {185--204},
  year      = {2019},
  url       = {https://doi.org/10.1016/j.ijar.2018.11.002},
  doi       = {10.1016/j.ijar.2018.11.002},
  timestamp = {Fri, 21 Feb 2020 13:12:23 +0100},
  biburl    = {https://dblp.org/rec/journals/ijar/CoxLV19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{6145622,
  author={C. B. {Browne} and E. {Powley} and D. {Whitehouse} and S. M. {Lucas} and P. I. {Cowling} and P. {Rohlfshagen} and S. {Tavener} and D. {Perez} and S. {Samothrakis} and S. {Colton}},
  journal={IEEE Transactions on Computational Intelligence and AI in Games}, 
  title={A Survey of {Monte Carlo} Tree Search Methods}, 
  year={2012},
  volume={4},
  number={1},
  pages={1-43}
}

@article{MuZero,
  title={{Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model}},
  author={Julian Schrittwieser and Ioannis Antonoglou and Thomas Hubert and Karen Simonyan and Laurent Sifre and Simon Schmitt and Arthur Guez and Edward Lockhart and Demis Hassabis and Thore Graepel and Timothy P. Lillicrap and David Silver},
  journal={ArXiv},
  year={2019},
  volume={abs/1911.08265}
}

@article{dacosta2020relationship,
  author    = {Lancelot {Da Costa} and Noor Sajid and Thomas Parr and Karl Friston and Ryan Smith},  
  title     = {The relationship between dynamic programming and active inference: the discrete, finite-horizon case},
  journal   = {arXiv},
  eprint    = {2009.08111},
  primaryClass={cs.AI},
  year      = {2020}
}

@ARTICLE{AI_DISCRET,
AUTHOR={Friston, Karl and Schwartenbeck, Philipp and Fitzgerald, Thomas and Moutoussis, Michael and Behrens, Tim and Dolan, Raymond},   
TITLE={The anatomy of choice: active inference and agency},      
JOURNAL={Frontiers in Human Neuroscience},      
VOLUME={7},     
PAGES={598},    
YEAR={2013},      
URL={https://www.frontiersin.org/article/10.3389/fnhum.2013.00598},       
DOI={10.3389/fnhum.2013.00598},      
ISSN={1662-5161},   
ABSTRACT={This paper considers agency in the setting of embodied or active inference. In brief, we associate a sense of agency with prior beliefs about action and ask what sorts of beliefs underlie optimal behavior. In particular, we consider prior beliefs that action minimizes the Kullback–Leibler (KL) divergence between desired states and attainable states in the future. This allows one to formulate bounded rationality as approximate Bayesian inference that optimizes a free energy bound on model evidence. We show that constructs like expected utility, exploration bonuses, softmax choice rules and optimism bias emerge as natural consequences of this formulation. Previous accounts of active inference have focused on predictive coding and Bayesian filtering schemes for minimizing free energy. Here, we consider variational Bayes as an alternative scheme that provides formal constraints on the computational anatomy of inference and action—constraints that are remarkably consistent with neuroanatomy. Furthermore, this scheme contextualizes optimal decision theory and economic (utilitarian) formulations as pure inference problems. For example, expected utility theory emerges as a special case of free energy minimization, where the sensitivity or inverse temperature (of softmax functions and quantal response equilibria) has a unique and Bayes-optimal solution—that minimizes free energy. This sensitivity corresponds to the precision of beliefs about behavior, such that attainable goals are afforded a higher precision or confidence. In turn, this means that optimal behavior entails a representation of confidence about outcomes that are under an agent's control.}
}

@article{millidge2020expected,
    author = {Millidge, Beren and Tschantz, Alexander and Buckley, Christopher L.},
    title = "{Whence the Expected Free Energy?}",
    journal = {Neural Computation},
    volume = {33},
    number = {2},
    pages = {447-482},
    year = {2021},
    month = {02},
    abstract = "{The expected free energy (EFE) is a central quantity in the theory of active inference. It is the quantity that all active inference agents are mandated to minimize through action, and its decomposition into extrinsic and intrinsic value terms is key to the balance of exploration and exploitation that active inference agents evince. Despite its importance, the mathematical origins of this quantity and its relation to the variational free energy (VFE) remain unclear. In this letter, we investigate the origins of the EFE in detail and show that it is not simply ”the free energy in the future.” We present a functional that we argue is the natural extension of the VFE but actively discourages exploratory behavior, thus demonstrating that exploration does not directly follow from free energy minimization into the future. We then develop a novel objective, the free energy of the expected future (FEEF), which possesses both the epistemic component of the EFE and an intuitive mathematical grounding as the divergence between predicted and desired futures.}",
    issn = {0899-7667},
    doi = {10.1162/neco_a_01354},
    url = {https://doi.org/10.1162/neco\_a\_01354},
    eprint = {https://direct.mit.edu/neco/article-pdf/33/2/447/1896836/neco\_a\_01354.pdf},
}

@article{DeepAI,
title = "Deep active inference as variational policy gradients",
journal = "Journal of Mathematical Psychology",
volume = "96",
pages = "102348",
year = "2020",
issn = "0022-2496",
doi = "https://doi.org/10.1016/j.jmp.2020.102348",
url = "http://www.sciencedirect.com/science/article/pii/S0022249620300298",
author = "Beren Millidge",
keywords = "Active inference, Predictive processing, Neural networks, Policy gradients, Reinforcement learning",
}

@article{PixelBasedAI,
  title={End-to-End Pixel-Based Deep Active Inference for Body Perception and Action},
  author={Cansu Sancaktar and Pablo Lanillos},
  journal={ArXiv},
  year={2020},
  volume={abs/2001.05847}
}

@article{DBLP:journals/corr/abs-1801-01290,
  author    = {Tuomas Haarnoja and
               Aurick Zhou and
               Pieter Abbeel and
               Sergey Levine},
  title     = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning
               with a Stochastic Actor},
  journal   = {CoRR},
  volume    = {abs/1801.01290},
  year      = {2018},
  url       = {http://arxiv.org/abs/1801.01290},
  archivePrefix = {arXiv},
  eprint    = {1801.01290},
  timestamp = {Mon, 13 Aug 2018 16:48:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1801-01290.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DeepRL,
  author    = {Volodymyr Mnih and
               Koray Kavukcuoglu and
               David Silver and
               Alex Graves and
               Ioannis Antonoglou and
               Daan Wierstra and
               Martin A. Riedmiller},
  title     = {Playing Atari with Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1312.5602},
  year      = {2013},
  url       = {http://arxiv.org/abs/1312.5602},
  archivePrefix = {arXiv},
  eprint    = {1312.5602},
  timestamp = {Mon, 13 Aug 2018 16:47:42 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/MnihKSGAWR13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DDQN,
  author    = {Hado van Hasselt and
               Arthur Guez and
               David Silver},
  editor    = {Dale Schuurmans and
               Michael P. Wellman},
  title     = {Deep Reinforcement Learning with Double Q-Learning},
  booktitle = {Proceedings of the Thirtieth {AAAI} Conference on Artificial Intelligence,
               February 12-17, 2016, Phoenix, Arizona, {USA}},
  pages     = {2094--2100},
  publisher = {{AAAI} Press},
  year      = {2016},
  url       = {http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12389},
  timestamp = {Wed, 10 Feb 2021 08:45:05 +0100},
  biburl    = {https://dblp.org/rec/conf/aaai/HasseltGS16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lample2016playing,
  author    = {Guillaume Lample and
               Devendra Singh Chaplot},
  editor    = {Satinder P. Singh and
               Shaul Markovitch},
  title     = {Playing {FPS} Games with Deep Reinforcement Learning},
  booktitle = {Proceedings of the Thirty-First {AAAI} Conference on Artificial Intelligence,
               February 4-9, 2017, San Francisco, California, {USA}},
  pages     = {2140--2146},
  publisher = {{AAAI} Press},
  year      = {2017},
  url       = {http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14456},
  timestamp = {Wed, 10 Feb 2021 08:43:37 +0100},
  biburl    = {https://dblp.org/rec/conf/aaai/LampleC17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{BOTVINICK2019408,
title = "Reinforcement Learning, Fast and Slow",
journal = "Trends in Cognitive Sciences",
volume = "23",
number = "5",
pages = "408 - 422",
year = "2019",
issn = "1364-6613",
doi = "https://doi.org/10.1016/j.tics.2019.02.006",
url = "http://www.sciencedirect.com/science/article/pii/S1364661319300610",
author = "Matthew Botvinick and Sam Ritter and Jane X. Wang and Zeb Kurth-Nelson and Charles Blundell and Demis Hassabis",
abstract = "Deep reinforcement learning (RL) methods have driven impressive advances in artificial intelligence in recent years, exceeding human performance in domains ranging from Atari to Go to no-limit poker. This progress has drawn the attention of cognitive scientists interested in understanding human learning. However, the concern has been raised that deep RL may be too sample-inefficient – that is, it may simply be too slow – to provide a plausible model of how humans learn. In the present review, we counter this critique by describing recently developed techniques that allow deep RL to operate more nimbly, solving problems much more quickly than previous methods. Although these techniques were developed in an AI context, we propose that they may have rich implications for psychology and neuroscience. A key insight, arising from these AI methods, concerns the fundamental connection between fast RL and slower, more incremental forms of learning."
}




























@article{mnist,
  added-at = {2010-06-28T21:16:30.000+0200},
  author = {LeCun, Yann and Cortes, Corinna},
  biburl = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/mhwombat},
  groups = {public},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  interhash = {21b9d0558bd66279df9452562df6e6f3},
  intrahash = {935bad99fa1f65e03c25b315aa3c1032},
  keywords = {MSc _checked character_recognition mnist network neural},
  lastchecked = {2016-01-14 14:24:11},
  timestamp = {2016-07-12T19:25:30.000+0200},
  title = {{MNIST} handwritten digit database},
  url = {http://yann.lecun.com/exdb/mnist/},
  username = {mhwombat},
  year = 2010
}

@misc{levine2018reinforcement,
      title={Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review}, 
      author={Sergey Levine},
      year={2018},
      eprint={1805.00909},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{SVI1,
  author    = {Wim Wiegerinck},
  editor    = {Craig Boutilier and
               Mois{\'{e}}s Goldszmidt},
  title     = {Variational Approximations between Mean Field Theory and the Junction
               Tree Algorithm},
  booktitle = {{UAI} '00: Proceedings of the 16th Conference in Uncertainty in Artificial
               Intelligence, Stanford University, Stanford, California, USA, June
               30 - July 3, 2000},
  pages     = {626--633},
  publisher = {Morgan Kaufmann},
  year      = {2000},
  url       = {https://dslpitt.org/uai/displayArticleDetails.jsp?mmnu=1\&smnu=2\&article\_id=73\&proceeding\_id=16},
  timestamp = {Wed, 06 May 2015 15:02:56 +0200},
  biburl    = {https://dblp.org/rec/conf/uai/Wiegerinck00.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{SVI2,
  author    = {Eric P. Xing and
               Michael I. Jordan and
               Stuart J. Russell},
  title     = {A Generalized Mean Field Algorithm for Variational Inference in Exponential
               Families},
  journal   = {CoRR},
  volume    = {abs/1212.2512},
  year      = {2012},
  url       = {http://arxiv.org/abs/1212.2512},
  archivePrefix = {arXiv},
  eprint    = {1212.2512},
  timestamp = {Mon, 13 Aug 2018 16:46:42 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1212-2512.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{910572,
  author={F. R. {Kschischang} and B. J. {Frey} and H. -. {Loeliger}},
  journal={IEEE Transactions on Information Theory}, 
  title={Factor graphs and the sum-product algorithm}, 
  year={2001},
  volume={47},
  number={2},
  pages={498-519}
}
  
@Article{Berridge2007,
author={Berridge, Kent C.},
title={The debate over dopamine's role in reward: the case for incentive salience},
journal={Psychopharmacology},
year={2007},
month={Apr},
day={01},
volume={191},
number={3},
pages={391-431},
abstract={Debate continues over the precise causal contribution made by mesolimbic dopamine systems to reward. There are three competing explanatory categories: `liking', learning, and `wanting'. Does dopamine mostly mediate the hedonic impact of reward (`liking')? Does it instead mediate learned predictions of future reward, prediction error teaching signals and stamp in associative links (learning)? Or does dopamine motivate the pursuit of rewards by attributing incentive salience to reward-related stimuli (`wanting')? Each hypothesis is evaluated here, and it is suggested that the incentive salience or `wanting' hypothesis of dopamine function may be consistent with more evidence than either learning or `liking'. In brief, recent evidence indicates that dopamine is neither necessary nor sufficient to mediate changes in hedonic `liking' for sensory pleasures. Other recent evidence indicates that dopamine is not needed for new learning, and not sufficient to directly mediate learning by causing teaching or prediction signals. By contrast, growing evidence indicates that dopamine does contribute causally to incentive salience. Dopamine appears necessary for normal `wanting', and dopamine activation can be sufficient to enhance cue-triggered incentive salience. Drugs of abuse that promote dopamine signals short circuit and sensitize dynamic mesolimbic mechanisms that evolved to attribute incentive salience to rewards. Such drugs interact with incentive salience integrations of Pavlovian associative information with physiological state signals. That interaction sets the stage to cause compulsive `wanting' in addiction, but also provides opportunities for experiments to disentangle `wanting', `liking', and learning hypotheses. Results from studies that exploited those opportunities are described here.},
issn={1432-2072},
doi={10.1007/s00213-006-0578-x},
url={https://doi.org/10.1007/s00213-006-0578-x}
}

@article {Schultz1593,
	author = {Schultz, Wolfram and Dayan, Peter and Montague, P. Read},
	title = {A Neural Substrate of Prediction and Reward},
	volume = {275},
	number = {5306},
	pages = {1593--1599},
	year = {1997},
	doi = {10.1126/science.275.5306.1593},
	publisher = {American Association for the Advancement of Science},
	abstract = {The capacity to predict future events permits a creature to detect, model, and manipulate the causal structure of its interactions with its environment. Behavioral experiments suggest that learning is driven by changes in the expectations about future salient events such as rewards and punishments. Physiological work has recently complemented these studies by identifying dopaminergic neurons in the primate whose fluctuating output apparently signals changes or errors in the predictions of future salient and rewarding events. Taken together, these findings can be understood through quantitative theories of adaptive optimizing control.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/275/5306/1593},
	eprint = {https://science.sciencemag.org/content/275/5306/1593.full.pdf},
	journal = {Science}
}

@misc{koller2009probabilistic,
  title={Probabilistic Graphical Models, Massachusetts},
  author={Koller, D and Friedman, N},
  year={2009},
  publisher={MIT Press USA}
}

@Article{BP_and_DC,
author={Yedidia, Jonathan S.},
title={Message-Passing Algorithms for Inference and Optimization},
journal={Journal of Statistical Physics},
year={2011},
month={Nov},
day={01},
volume={145},
number={4},
pages={860-890},
abstract={Message-passing algorithms can solve a wide variety of optimization, inference, and constraint satisfaction problems. The algorithms operate on factor graphs that visually represent and specify the structure of the problems. After describing some of their applications, I survey the family of belief propagation (BP) algorithms, beginning with a detailed description of the min-sum algorithm and its exactness on tree factor graphs, and then turning to a variety of more sophisticated BP algorithms, including free-energy based BP algorithms, ``splitting'' BP algorithms that generalize ``tree-reweighted'' BP, and the various BP algorithms that have been proposed to deal with problems with continuous variables.},
issn={1572-9613},
doi={10.1007/s10955-011-0384-7},
url={https://doi.org/10.1007/s10955-011-0384-7}
}

@Article{believe,
author = {Friston, Karl J. and Parr, Thomas and de Vries, Bert},
title = {The graphical brain: Belief propagation and active inference},
journal = {Network Neuroscience},
volume = {1},
number = {4},
pages = {381-414},
year = {2017},
doi = {10.1162/NETN\_a\_00018},
URL = {https://doi.org/10.1162/NETN_a_00018},
eprint = {https://doi.org/10.1162/NETN_a_00018},
abstract = { This paper considers functional integration in the brain from a computational perspective. We ask what sort of neuronal message passing is mandated by active inference—and what implications this has for context-sensitive connectivity at microscopic and macroscopic levels. In particular, we formulate neuronal processing as belief propagation under deep generative models. Crucially, these models can entertain both discrete and continuous states, leading to distinct schemes for belief updating that play out on the same (neuronal) architecture. Technically, we use Forney (normal) factor graphs to elucidate the requisite message passing in terms of its form and scheduling. To accommodate mixed generative models (of discrete and continuous states), one also has to consider link nodes or factors that enable discrete and continuous representations to talk to each other. When mapping the implicit computational architecture onto neuronal connectivity, several interesting features emerge. For example, Bayesian model averaging and comparison, which link discrete and continuous states, may be implemented in thalamocortical loops. These and other considerations speak to a computational connectome that is inherently state dependent and self-organizing in ways that yield to a principled (variational) account. We conclude with simulations of reading that illustrate the implicit neuronal message passing, with a special focus on how discrete (semantic) representations inform, and are informed by, continuous (visual) sampling of the sensorium.}
}

@Article{Friston2010,
author={Friston, Karl},
title={The free-energy principle: a unified brain theory?},
journal={Nature Reviews Neuroscience},
year={2010},
month={Feb},
day={01},
volume={11},
number={2},
pages={127-138},
abstract={Adaptive agents must occupy a limited repertoire of states and therefore minimize the long-term average of surprise associated with sensory exchanges with the world. Minimizing surprise enables them to resist a natural tendency to disorder.Surprise rests on predictions about sensations, which depend on an internal generative model of the world. Although surprise cannot be measured directly, a free-energy bound on surprise can be, suggesting that agents minimize free energy by changing their predictions (perception) or by changing the predicted sensory inputs (action).Perception optimizes predictions by minimizing free energy with respect to synaptic activity (perceptual inference), efficacy (learning and memory) and gain (attention and salience). This furnishes Bayes-optimal (probabilistic) representations of what caused sensations (providing a link to the Bayesian brain hypothesis).Bayes-optimal perception is mathematically equivalent to predictive coding and maximizing the mutual information between sensations and the representations of their causes. This is a probabilistic generalization of the principle of efficient coding (the infomax principle) or the minimum-redundancy principle.Learning under the free-energy principle can be formulated in terms of optimizing the connection strengths in hierarchical models of the sensorium. This rests on associative plasticity to encode causal regularities and appeals to the same synaptic mechanisms as those underlying cell assembly formation.Action under the free-energy principle reduces to suppressing sensory prediction errors that depend on predicted (expected or desired) movement trajectories. This provides a simple account of motor control, in which action is enslaved by perceptual (proprioceptive) predictions.Perceptual predictions rest on prior expectations about the trajectory or movement through the agent's state space. These priors can be acquired (as empirical priors during hierarchical inference) or they can be innate (epigenetic) and therefore subject to selective pressure.Predicted motion or state transitions realized by action correspond to policies in optimal control theory and reinforcement learning. In this context, value is inversely proportional to surprise (and implicitly free energy), and rewards correspond to innate priors that constrain policies.},
issn={1471-0048},
doi={10.1038/nrn2787},
url={https://doi.org/10.1038/nrn2787}
}

@misc{AITS_PRACTICE,
      title={Branching Time Active Inference: empirical study}, 
      author={Th{\'{e}}ophile Champion and Howard Bowman and Marek Grze{\'{s}}},
      year={2021},
      archivePrefix={arXiv},
      url = {https://arxiv.org/abs/2111.11276},
      note = {Available at \url{https://arxiv.org/abs/2111.11276}.}
}

@misc{TUTO_AI_RYAN,
title={A Step-by-Step Tutorial on Active Inference and its Application to Empirical Data},
author={Smith, Ryan and Friston, Karl J. and Whyte, Christopher J.},
year={2021},
archivePrefix={psyarxiv},
URL = {https://psyarxiv.com/b4jm6/}
}

@article{BUCKLEY201755,
title = "The free energy principle for action and perception: A mathematical review",
journal = "Journal of Mathematical Psychology",
volume = "81",
pages = "55 - 79",
year = "2017",
issn = "0022-2496",
doi = "https://doi.org/10.1016/j.jmp.2017.09.004",
author = "Christopher L. Buckley and Chang Sub Kim and Simon McGregor and Anil K. Seth",
keywords = "Free energy principle, Perception, Action, Inference, Bayesian brain, Agent-based model",
}

@ARTICLE{active_vision,  author={D. {Ognibene} and G. {Baldassare}},  journal={IEEE Transactions on Autonomous Mental Development},   title={Ecological Active Vision: Four Bioinspired Principles to Integrate Bottom–Up and Adaptive Top–Down Attention Tested With a Simple Camera-Arm Robot},   year={2015},  volume={7},  number={1},  pages={3-25},}

@article{bellman1954,
author = "Bellman, Richard",
fjournal = "Bulletin of the American Mathematical Society",
journal = "Bull. Amer. Math. Soc.",
month = "11",
number = "6",
pages = "503--515",
publisher = "American Mathematical Society",
title = "The theory of dynamic programming",
url = "https://projecteuclid.org:443/euclid.bams/1183519147",
volume = "60",
year = "1954"
}

@article{10000025057,
author="Watkins, C. J. C. H.",
title="Learning from delayed rewards",
journal="PhD thesis, Cambridge University",
ISSN="",
publisher="",
year="1989",
month="",
volume="",
number="",
pages="",
URL="https://ci.nii.ac.jp/naid/10000025057/en/",
DOI="",
}

@incollection{NIPS2010_3964,
title = {Double {Q}-learning},
author = {Hado V. Hasselt},
booktitle = {Advances in Neural Information Processing Systems 23},
editor = {J. D. Lafferty and C. K. I. Williams and J. Shawe-Taylor and R. S. Zemel and A. Culotta},
pages = {2613--2621},
year = {2010},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/3964-double-q-learning.pdf}
}

@Article{adaptive_coding,
author={Duncan, John},
title={An adaptive coding model of neural function in prefrontal cortex},
journal={Nature Reviews Neuroscience},
year={2001},
month={Nov},
day={01},
volume={2},
number={11},
pages={820-829},
issn={1471-0048},
doi={10.1038/35097575},
url={https://doi.org/10.1038/35097575}
}

@inproceedings{Rish2001AnES,
  title={An empirical study of the naive {Bayes} classifier},
  author={Irina Rish},
  year={2001}
}

@inproceedings{Metsis06spamfiltering,
    author = {Vangelis Metsis and et al.},
    title = {Spam Filtering with Naive {Bayes} -- Which Naive {Bayes}?},
    booktitle = {THIRD CONFERENCE ON EMAIL AND ANTI-SPAM (CEAS},
    year = {2006},
    publisher = {}
}

@article{LDA,
author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
title = {Latent {Dirichlet} Allocation},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
month = mar,
pages = {993–1022},
numpages = {30}
}

@ARTICLE{HMM,
  author={L. {Rabiner} and B. {Juang}},
  journal={IEEE ASSP Magazine}, 
  title={An introduction to hidden {Markov} models}, 
  year={1986},
  volume={3},
  number={1},
  pages={4-16}
}

@Book{Sampling_Chapter,
author = { Christopher Bishop },
title = {Pattern Recognition and Machine Learning},
publisher = {Springer},
year = 2006,
pages = {738},
chapter = 11
}

@article{occam,
title = "Occam's Razor",
journal = "Information Processing Letters",
volume = "24",
number = "6",
pages = "377 - 380",
year = "1987",
issn = "0020-0190",
doi = "https://doi.org/10.1016/0020-0190(87)90114-1",
url = "http://www.sciencedirect.com/science/article/pii/0020019087901141",
author = "Anselm Blumer and Andrzej Ehrenfeucht and David Haussler and Manfred K. Warmuth",
keywords = "Machine learning, induction, inductive inference, Occam's Razor, methodology of science",
abstract = "We show that a polynomial learning algorithm, as defined by Valiant (1984), is obtained whenever there exists a polynomial-time method of producing, for any sequence of observations, a nearly minimum hypothesis that is consistent with these observations."
}

@inproceedings{KL_CONTROL,
author = {Rawlik, Konrad and Toussaint, Marc and Vijayakumar, Sethu},
title = {On Stochastic Optimal Control and Reinforcement Learning by Approximate Inference (Extended Abstract)},
year = {2013},
isbn = {9781577356332},
publisher = {AAAI Press},
booktitle = {Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence},
pages = {3052–3056},
numpages = {5},
location = {Beijing, China},
series = {IJCAI ’13}
}

@article{EPISTEMIC_VALUE,
author = {Karl Friston and Francesco Rigoli and Dimitri Ognibene and Christoph Mathys and Thomas Fitzgerald and Giovanni Pezzulo},
title = {Active inference and epistemic value},
journal = {Cognitive Neuroscience},
volume = {6},
number = {4},
pages = {187-214},
year  = {2015},
publisher = {Routledge},
doi = {10.1080/17588928.2015.1020053},
note ={PMID: 25689102},
URL = { https://doi.org/10.1080/17588928.2015.1020053},
eprint = { https://doi.org/10.1080/17588928.2015.1020053}
}

@Article{SC2,
author={Vinyals, Oriol
and Babuschkin, Igor
and Czarnecki, Wojciech M.
and Mathieu, Micha{\"e}l
and Dudzik, Andrew
and Chung, Junyoung
and Choi, David H.
and Powell, Richard
and Ewalds, Timo
and Georgiev, Petko
and Oh, Junhyuk
and Horgan, Dan
and Kroiss, Manuel
and Danihelka, Ivo
and Huang, Aja
and Sifre, Laurent
and Cai, Trevor
and Agapiou, John P.
and Jaderberg, Max
and Vezhnevets, Alexander S.
and Leblond, R{\'e}mi
and Pohlen, Tobias
and Dalibard, Valentin
and Budden, David
and Sulsky, Yury
and Molloy, James
and Paine, Tom L.
and Gulcehre, Caglar
and Wang, Ziyu
and Pfaff, Tobias
and Wu, Yuhuai
and Ring, Roman
and Yogatama, Dani
and W{\"u}nsch, Dario
and McKinney, Katrina
and Smith, Oliver
and Schaul, Tom
and Lillicrap, Timothy
and Kavukcuoglu, Koray
and Hassabis, Demis
and Apps, Chris
and Silver, David},
title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
journal={Nature},
year={2019},
month={Nov},
day={01},
volume={575},
number={7782},
pages={350-354},
abstract={Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1--3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8{\%} of officially ranked human players.},
issn={1476-4687},
doi={10.1038/s41586-019-1724-z},
url={https://doi.org/10.1038/s41586-019-1724-z}
}

@inproceedings{MAXQ,
  author    = {Thomas G. Dietterich},
  title     = {The {MAXQ} Method for Hierarchical Reinforcement Learning},
  booktitle = {Proceedings of the Fifteenth International Conference on Machine Learning
               {(ICML} 1998), Madison, Wisconsin, USA, July 24-27, 1998},
  pages     = {118--126},
  year      = {1998},
  crossref  = {DBLP:conf/icml/1998},
  timestamp = {Thu, 30 Jun 2011 10:34:12 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/Dietterich98.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@proceedings{DBLP:conf/icml/1998,
  editor    = {Jude W. Shavlik},
  title     = {Proceedings of the Fifteenth International Conference on Machine Learning
               {(ICML} 1998), Madison, Wisconsin, USA, July 24-27, 1998},
  publisher = {Morgan Kaufmann},
  year      = {1998},
  isbn      = {1-55860-556-8},
  timestamp = {Thu, 30 Jun 2011 10:34:12 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/1998.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{BHRL,
title = {{Bayesian} Hierarchical Reinforcement Learning},
author = {Feng Cao and Soumya Ray},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {73--81},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4752-bayesian-hierarchical-reinforcement-learning.pdf}
}

@article{FRISTON2018486,
title = "Deep temporal models and active inference",
journal = "Neuroscience \& Biobehavioral Reviews",
volume = "90",
pages = "486 - 501",
year = "2018",
issn = "0149-7634",
doi = "https://doi.org/10.1016/j.neubiorev.2018.04.004",
url = "http://www.sciencedirect.com/science/article/pii/S0149763418302525",
author = "Karl J. Friston and Richard Rosch and Thomas Parr and Cathy Price and Howard Bowman",
keywords = "Active inference, Bayesian, Hierarchical, Reading, Violation, Free energy, P300, MMN"
}

@article{Watkins1992,
	abstract = {
$$\mathcal\{Q\}$$  -learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem for     $$\mathcal\{Q\}$$  -learning based on that outlined in Watkins (1989). We show that     $$\mathcal\{Q\}$$  -learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where many     $$\mathcal\{Q\}$$   values can be changed each iteration, rather than just one.
},
	affiliation = {25b Framfield Road, N5 1UU Highbury, London, England; Centre for Cognitive Science, University of Edinburgh},
	author = {Watkins, Christopher J.C.H. and Dayan, Peter},
	copyright = {Kluwer Academic Publishers},
	doi = {10.1023/A:1022676722315},
	journal = {Machine Learning},
	keywords = {$$\mathcal\{Q\}$$  -learning; reinforcement learning; temporal differences; asynchronous dynamic programming},
	language = {English},
	number = {3-4},
	pages = {279-292},
	title = {Technical Note: Q-Learning},
	volume = {8},
	year = {1992},
}

@article{hippocampus_and_ghrelin,
author = {Kojima, Masayasu and Kangawa, Kenji},
title = {Ghrelin: Structure and Function},
journal = {Physiological Reviews},
volume = {85},
number = {2},
pages = {495-522},
year = {2005},
doi = {10.1152/physrev.00012.2004},
note ={PMID: 15788704},
URL = {https://doi.org/10.1152/physrev.00012.2004},
eprint = {https://doi.org/10.1152/physrev.00012.2004},
abstract = { Small synthetic molecules called growth hormone secretagogues (GHSs) stimulate the release of growth hormone (GH) from the pituitary. They act through the GHS-R, a G protein-coupled receptor whose ligand has only been discovered recently. Using a reverse pharmacology paradigm with a stable cell line expressing GHS-R, we purified an endogenous ligand for GHS-R from rat stomach and named it “ghrelin,” after a word root (“ghre”) in Proto-Indo-European languages meaning “grow.” Ghrelin is a peptide hormone in which the third amino acid, usually a serine but in some species a threonine, is modified by a fatty acid; this modification is essential for ghrelin's activity. The discovery of ghrelin indicates that the release of GH from the pituitary might be regulated not only by hypothalamic GH-releasing hormone, but also by ghrelin derived from the stomach. In addition, ghrelin stimulates appetite by acting on the hypothalamic arcuate nucleus, a region known to control food intake. Ghrelin is orexigenic; it is secreted from the stomach and circulates in the bloodstream under fasting conditions, indicating that it transmits a hunger signal from the periphery to the central nervous system. Taking into account all these activities, ghrelin plays important roles for maintaining GH release and energy homeostasis in vertebrates.}
}

@online{oleg,
  author = {Oleg Solopchuk},
  title = {Tutorial on Active Inference},
  year = 2018,
  url = {https://medium.com/@solopchuk/tutorial-on-active-inference-30edcf50f5dc},
  urldate = {2018-09-04}
}

@ARTICLE{BMR,
       author = {{Friston}, Karl and {Parr}, Thomas and {Zeidman}, Peter},
        title = "{{Bayesian} model reduction}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Methodology},
         year = 2018,
        month = may,
          eid = {arXiv:1805.07092},
        pages = {arXiv:1805.07092},
archivePrefix = {arXiv},
       eprint = {1805.07092},
 primaryClass = {stat.ME},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180507092F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article {BME,
	author = {Smith, Ryan and Schwartenbeck, Philipp and Parr, Thomas and Friston, Karl J.},
	title = {An active inference approach to modeling concept learning},
	elocation-id = {633677},
	year = {2019},
	doi = {10.1101/633677},
	publisher = {Cold Spring Harbor Laboratory},
	URL = {https://www.biorxiv.org/content/early/2019/05/23/633677},
	eprint = {https://www.biorxiv.org/content/early/2019/05/23/633677.full.pdf},
	journal = {bioRxiv}
}

@article{Pang_2019,
   title={On Reinforcement Learning for Full-Length Game of StarCraft},
   volume={33},
   ISSN={2159-5399},
   url={http://dx.doi.org/10.1609/aaai.v33i01.33014691},
   DOI={10.1609/aaai.v33i01.33014691},
   journal={Proceedings of the AAAI Conference on Artificial Intelligence},
   publisher={Association for the Advancement of Artificial Intelligence (AAAI)},
   author={Pang, Zhen-Jia and Liu, Ruo-Ze and Meng, Zhou-Yu and Zhang, Yi and Yu, Yang and Lu, Tong},
   year={2019},
   month={Jul},
   pages={4691–4698}
}

@misc{VAE,
    title={Tutorial on Variational Autoencoders},
    author={Carl Doersch},
    year={2016},
    eprint={1606.05908},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@inproceedings{beta-VAE,
  author    = {Irina Higgins and
               Lo{\"{\i}}c Matthey and
               Arka Pal and
               Christopher Burgess and
               Xavier Glorot and
               Matthew Botvinick and
               Shakir Mohamed and
               Alexander Lerchner},
  title     = {beta-{VAE}: Learning Basic Visual Concepts with a Constrained Variational
               Framework},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  year      = {2017},
  crossref  = {DBLP:conf/iclr/2017},
  url       = {https://openreview.net/forum?id=Sy2fzU9gl},
  timestamp = {Thu, 25 Jul 2019 14:25:41 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/HigginsMPBGBML17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@proceedings{DBLP:conf/iclr/2017,
  title     = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/group?id=ICLR.cc/2017/conference},
  timestamp = {Thu, 04 Apr 2019 13:20:07 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/2017.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{designpatterns,
    author = {Dietmar Kühl},
    title = {Design patterns for the implementation of graph algorithms},
    booktitle = {MASTER’S THESIS, TECHNISCHE UNIVERSITÄT},
    year = {1996},
    publisher = {}
}

@inproceedings{kar60614,
       booktitle = {Sixteenth International Conference on Autonomous Agents and Multiagent Sytems (AAMAS 2017)},
           month = {May},
           title = {Reward Shaping in Episodic Reinforcement Learning},
          author = {Marek Grzes},
       publisher = {ACM},
            year = {2017},
           pages = {565--573},
        keywords = {Reward structures for learning; Multiagent learning; Reward shaping; Reinforcement learning},
             url = {https://kar.kent.ac.uk/60614/},
        abstract = {Recent advancements in reinforcement learning confirm that reinforcement learning techniques can solve large scale problems leading to high quality autonomous decision making. It is a matter of time until we will see large scale applications of reinforcement learning in various sectors, such as healthcare and cyber-security, among others. However, reinforcement learning can be time-consuming because the learning algorithms have to determine the long term consequences of their actions using delayed feedback or rewards. Reward shaping is a method of incorporating domain knowledge into reinforcement learning so that the algorithms are guided faster towards more promising solutions. Under an overarching theme of episodic reinforcement learning, this paper shows a unifying analysis of potential-based reward shaping which leads to new theoretical insights into reward shaping in both model-free and model-based algorithms, as well as in multi-agent reinforcement learning.}
}

@article{10010902373,
author="NG, A. Y.",
title="Policy invariance under reward transformations: Theory and application to reward shaping",
journal="Proceedings of the 16th International Conference on Machine Learning",
ISSN="",
publisher="",
year="1999",
month="",
volume="",
number="",
pages="",
URL="https://ci.nii.ac.jp/naid/10010902373/en/",
DOI="",
}