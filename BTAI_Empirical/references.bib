@article{veness2011variance,
  title={Variance reduction in monte-carlo tree search},
  author={Veness, Joel and Lanctot, Marc and Bowling, Michael},
  journal={Advances in Neural Information Processing Systems},
  volume={24},
  year={2011}
}

@INPROCEEDINGS{STATES_AGGREG,
  author={Zhiyuan Ren and Krogh, B.H.},
  booktitle={Proceedings of the 41st IEEE Conference on Decision and Control, 2002.}, 
  title={State aggregation in Markov decision processes}, 
  year={2002},
  volume={4},
  number={},
  pages={3819-3824 vol.4},
  doi={10.1109/CDC.2002.1184960}
}

@misc{OpenAI,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}

@article{PARTICLE_FILTER,
  title={A tutorial on particle filtering and smoothing: Fifteen years later},
  author={Doucet, Arnaud and Johansen, Adam M and others},
  journal={Handbook of nonlinear filtering},
  volume={12},
  number={656-704},
  pages={3},
  year={2009}
}

@article{POMCP,
  title={Monte-Carlo planning in large POMDPs},
  author={Silver, David and Veness, Joel},
  journal={Advances in neural information processing systems},
  volume={23},
  year={2010}
}

@misc{dsprites17,
author = {Loic Matthey and Irina Higgins and Demis Hassabis and Alexander Lerchner},
title = {dSprites: Disentanglement testing Sprites dataset},
howpublished= {https://github.com/deepmind/dsprites-dataset/},
year = "2017",
}

@article{BUTZ2019135,
title = {Learning, planning, and control in a monolithic neural event inference architecture},
journal = {Neural Networks},
volume = {117},
pages = {135-144},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019301339},
author = {Martin V. Butz and David Bilkey and Dania Humaidan and Alistair Knott and Sebastian Otte},
keywords = {Dynamical systems, Predictive model learning, Model predictive control, Active inference, Cognitive systems, Event cognition},
abstract = {We introduce REPRISE, a REtrospective and PRospective Inference SchEme, which learns temporal event-predictive models of dynamical systems. REPRISE infers the unobservable contextual event state and accompanying temporal predictive models that best explain the recently encountered sensorimotor experiences retrospectively. Meanwhile, it optimizes upcoming motor activities prospectively in a goal-directed manner. Here, REPRISE is implemented by a recurrent neural network (RNN), which learns temporal forward models of the sensorimotor contingencies generated by different simulated dynamic vehicles. The RNN is augmented with contextual neurons, which enable the encoding of distinct, but related, sensorimotor dynamics as compact event codes. We show that REPRISE concurrently learns to separate and approximate the encountered sensorimotor dynamics: it analyzes sensorimotor error signals adapting both internal contextual neural activities and connection weight values. Moreover, we show that REPRISE can exploit the learned model to induce goal-directed, model-predictive control, that is, approximate active inference: Given a goal state, the system imagines a motor command sequence optimizing it with the prospective objective to minimize the distance to the goal. The RNN activities thus continuously imagine the upcoming future and reflect on the recent past, optimizing the predictive model, the hidden neural state activities, and the upcoming motor activities. As a result, event-predictive neural encodings develop, which allow the invocation of highly effective and adaptive goal-directed sensorimotor control.}
}

@Article{Markovic2021,
author={Markovi{\'{c}}, Dimitrije
and Goschke, Thomas
and Kiebel, Stefan J.},
title={Meta-control of the exploration-exploitation dilemma emerges from probabilistic inference over a hierarchy of time scales},
journal={Cognitive, Affective, {\&} Behavioral Neuroscience},
year={2021},
month={Jun},
day={01},
volume={21},
number={3},
pages={509-533},
abstract={Cognitive control is typically understood as a set of mechanisms that enable humans to reach goals that require integrating the consequences of actions over longer time scales. Importantly, using routine behaviour or making choices beneficial only at short time scales would prevent one from attaining these goals. During the past two decades, researchers have proposed various computational cognitive models that successfully account for behaviour related to cognitive control in a wide range of laboratory tasks. As humans operate in a dynamic and uncertain environment, making elaborate plans and integrating experience over multiple time scales is computationally expensive. Importantly, it remains poorly understood how uncertain consequences at different time scales are integrated into adaptive decisions. Here, we pursue the idea that cognitive control can be cast as active inference over a hierarchy of time scales, where inference, i.e., planning, at higher levels of the hierarchy controls inference at lower levels. We introduce the novel concept of meta-control states, which link higher-level beliefs with lower-level policy inference. Specifically, we conceptualize cognitive control as inference over these meta-control states, where solutions to cognitive control dilemmas emerge through surprisal minimisation at different hierarchy levels. We illustrate this concept using the exploration-exploitation dilemma based on a variant of a restless multi-armed bandit task. We demonstrate that beliefs about contexts and meta-control states at a higher level dynamically modulate the balance of exploration and exploitation at the lower level of a single action. Finally, we discuss the generalisation of this meta-control concept to other control dilemmas.},
issn={1531-135X},
doi={10.3758/s13415-020-00837-x},
url={https://doi.org/10.3758/s13415-020-00837-x}
}

@article{PITTI2020242,
title = {Gated spiking neural network using Iterative Free-Energy Optimization and rank-order coding for structure learning in memory sequences (INFERNO GATE)},
journal = {Neural Networks},
volume = {121},
pages = {242-258},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.09.023},
url = {https://www.sciencedirect.com/science/article/pii/S089360801930303X},
author = {Alexandre Pitti and Mathias Quoy and Catherine Lavandier and Sofiane Boucenna},
keywords = {Free-energy, Rank-order coding, Structural learning, Gating mechanism, Prefrontal cortex, Language development},
abstract = {We present a framework based on iterative free-energy optimization with spiking neural networks for modeling the fronto-striatal system (PFC-BG) for the generation and recall of audio memory sequences. In line with neuroimaging studies carried out in the PFC, we propose a genuine coding strategy using the gain-modulation mechanism to represent abstract sequences based solely on the rank and location of items within them. Based on this mechanism, we show that we can construct a repertoire of neurons sensitive to the temporal structure in sequences from which we can represent any novel sequences. Free-energy optimization is then used to explore and to retrieve the missing indices of the items in the correct order for executive control and compositionality. We show that the gain-modulation mechanism permits the network to be robust to variabilities and to have long-term dependencies as it implements a gated recurrent neural network. This model, called Inferno Gate, is an extension of the neural architecture Inferno standing for Iterative Free-Energy Optimization of Recurrent Neural Networks with Gating or Gain-modulation. In experiments performed with an audio database of ten thousand MFCC vectors, Inferno Gate is capable of encoding efficiently and retrieving chunks of fifty items length. We then discuss the potential of our network to model the features of working memory in the PFC-BG loop for structural learning, goal-direction and hierarchical reinforcement learning.}
}

@article{sajid2021exploration,
      title={Exploration and preference satisfaction trade-off in reward-free learning}, 
      author={Noor Sajid and Panagiotis Tigas and Alexey Zakharov and Zafeirios Fountas and Karl Friston},
      year={2021},
      journal={arXiv},
      eprint={2106.04316},
      primaryClass={cs.AI}
}

@article{LargePOMDP,
  author    = {Domenico Maisto and
               Francesco Gregoretti and
               Karl J. Friston and
               Giovanni Pezzulo},
  title     = {Active Tree Search in Large POMDPs},
  journal   = {CoRR},
  volume    = {abs/2103.13860},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.13860},
  eprinttype = {arXiv},
  eprint    = {2103.13860},
  timestamp = {Wed, 07 Apr 2021 15:31:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-13860.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{9457162,
  author={Wirkuttis, Nadine and Tani, Jun},
  journal={IEEE Robotics and Automation Letters}, 
  title={Leading or Following? Dyadic Robot Imitative Interaction Using the Active Inference Framework}, 
  year={2021},
  volume={6},
  number={3},
  pages={6024-6031},
  doi={10.1109/LRA.2021.3090015}}
  
@ARTICLE{structure_learning,
AUTHOR={Smith, Ryan and Schwartenbeck, Philipp and Parr, Thomas and Friston, Karl J.},   
TITLE={An Active Inference Approach to Modeling Structure Learning: Concept Learning as an Example Case},      
JOURNAL={Frontiers in Computational Neuroscience},      
VOLUME={14},      
PAGES={41},
YEAR={2020},      
URL={https://www.frontiersin.org/article/10.3389/fncom.2020.00041},       
DOI={10.3389/fncom.2020.00041},      
ISSN={1662-5188},   
ABSTRACT={Within computational neuroscience, the algorithmic and neural basis of structure learning remains poorly understood. Concept learning is one primary example, which requires both a type of internal model expansion process (adding novel hidden states that explain new observations), and a model reduction process (merging different states into one underlying cause and thus reducing model complexity via meta-learning). Although various algorithmic models of concept learning have been proposed within machine learning and cognitive science, many are limited to various degrees by an inability to generalize, the need for very large amounts of training data, and/or insufficiently established biological plausibility. Using concept learning as an example case, we introduce a novel approach for modeling structure learning—and specifically state-space expansion and reduction—within the active inference framework and its accompanying neural process theory. Our aim is to demonstrate its potential to facilitate a novel line of active inference research in this area. The approach we lay out is based on the idea that a generative model can be equipped with extra (hidden state or cause) “slots” that can be engaged when an agent learns about novel concepts. This can be combined with a Bayesian model reduction process, in which any concept learning—associated with these slots—can be reset in favor of a simpler model with higher model evidence. We use simulations to illustrate this model's ability to add new concepts to its state space (with relatively few observations) and increase the granularity of the concepts it currently possesses. We also simulate the predicted neural basis of these processes. We further show that it can accomplish a simple form of “one-shot” generalization to new stimuli. Although deliberately simple, these simulation results highlight ways in which active inference could offer useful resources in developing neurocomputational models of structure learning. They provide a template for how future active inference research could apply this approach to real-world structure learning problems and assess the added utility it may offer.}
}

@article{rafetseder2013counterfactual,
  title={Counterfactual reasoning: From childhood to adulthood},
  author={Rafetseder, Eva and Schwitalla, Maria and Perner, Josef},
  journal={Journal of experimental child psychology},
  volume={114},
  number={3},
  pages={389--404},
  year={2013},
  publisher={Elsevier}
}

@inproceedings{concurrency_glabbeek,
author = {Glabbeek, Rob J. van},
title = {The Linear Time-Branching Time Spectrum (Extended Abstract)},
year = {1990},
isbn = {3540530487},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Proceedings of the Theories of Concurrency: Unification and Extension},
pages = {278–297},
numpages = {20},
series = {CONCUR '90}
}

@InProceedings{concurrency_glabbeek_2,
author="van Glabbeek, R. J.",
editor="Best, Eike",
title="The linear time --- Branching time spectrum {II}",
booktitle="CONCUR'93",
year="1993",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="66--81",
abstract="This paper studies semantic equivalences and preorders for sequential systems with silent moves, restricting attention to the ones that abstract from successful termination, stochastic and real-time aspects of the investigated systems, and the structure of the visible actions systems can perform. It provides a parameterized definition of such a preorder, such that most such preorders and equivalences found in the literature are obtained by a suitable instantiation of the parameters. Other instantiations yield preorders that combine properties from various semantics. Moreover, the approach shows several ways in which preorders that were originally only considered for systems without silent moves, most notably the ready simulation, can be generalized to an abstract setting, and how preorders that were originally only considered for for systems without divergence, such as the coupled simulation, can be extended to divergent systems. All preorders come with---or rather as---a modal characterization, and when possible also a relational characterization. The paper concludes with some pros and cons of the preorders.",
isbn="978-3-540-47968-0"
}

@book{concurrency_howard,
      author        = "Bowman, Howard",
      title         = "{Concurrency Theory: Calculi an Automata for Modelling
                       Untimed and Timed Concurrent Systems}",
      publisher     = "Springer",
      address       = "Dordrecht",
      year          = "2005",
      url           = "https://cds.cern.ch/record/1250124",
}

@book{SPM,
	place={Amsterdam},
	title={Statistical parametric mapping: the analysis of functional brain images},
	publisher={Elsevier},
	author={Friston, Karl J.},
	year={2007}
}

@misc{AITS_THEORY,
      title={Branching Time Active Inference: the theory and its generality}, 
      author={Th{\'{e}}ophile Champion and Howard Bowman and Marek Grze{\'{s}}},
      year={2021},
      archivePrefix={arXiv}
}

@article{Go,
  author    = {David Silver and
               Aja Huang and
               Chris J. Maddison and
               Arthur Guez and
               Laurent Sifre and
               George van den Driessche and
               Julian Schrittwieser and
               Ioannis Antonoglou and
               Vedavyas Panneershelvam and
               Marc Lanctot and
               Sander Dieleman and
               Dominik Grewe and
               John Nham and
               Nal Kalchbrenner and
               Ilya Sutskever and
               Timothy P. Lillicrap and
               Madeleine Leach and
               Koray Kavukcuoglu and
               Thore Graepel and
               Demis Hassabis},
  title     = {Mastering the game of Go with deep neural networks and tree search},
  journal   = {Nature},
  volume    = {529},
  number    = {7587},
  pages     = {484--489},
  year      = {2016},
  url       = {https://doi.org/10.1038/nature16961},
  doi       = {10.1038/nature16961},
  timestamp = {Wed, 14 Nov 2018 10:30:42 +0100},
  biburl    = {https://dblp.org/rec/journals/nature/SilverHMGSDSAPL16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{AI_VMP,
    author = {Champion, Théophile and Grześ, Marek and Bowman, Howard},
    title = "{Realizing Active Inference in Variational Message Passing: The Outcome-Blind Certainty Seeker}",
    journal = {Neural Computation},
    pages = {1-65},
    year = {2021},
    month = {07},
    abstract = "{Active inference is a state-of-the-art framework in neuroscience that offers a unified theory of brain function. It is also proposed as a framework for planning in AI. Unfortunately, the complex mathematics required to create new models can impede application of active inference in neuroscience and AI research. This letter addresses this problem by providing a complete mathematical treatment of the active inference framework in discrete time and state spaces and the derivation of the update equations for any new model. We leverage the theoretical connection between active inference and variational message passing as described by John Winn and Christopher M. Bishop in 2005. Since variational message passing is a well-defined methodology for deriving Bayesian belief update equations, this letter opens the door to advanced generative models for active inference. We show that using a fully factorized variational distribution simplifies the expected free energy, which furnishes priors over policies so that agents seek unambiguous states. Finally, we consider future extensions that support deep tree searches for sequential policy optimization based on structure learning and belief propagation.}",
    issn = {0899-7667},
    doi = {10.1162/neco_a_01422},
    url = {https://doi.org/10.1162/neco\_a\_01422},
    eprint = {https://direct.mit.edu/neco/article-pdf/doi/10.1162/neco\_a\_01422/1930278/neco\_a\_01422.pdf},
}

@ARTICLE{DeepAIwithMCMC,
       author = {{Fountas}, Zafeirios and {Sajid}, Noor and {Mediano}, Pedro A.~M. and {Friston}, Karl},
        title = "{Deep active inference agents using Monte-Carlo methods}",
      journal = {arXiv e-prints},
     keywords = {Quantitative Biology - Neurons and Cognition, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
         year = 2020,
        month = jun,
          eid = {arXiv:2006.04176},
        pages = {arXiv:2006.04176},
archivePrefix = {arXiv},
       eprint = {2006.04176},
 primaryClass = {q-bio.NC},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200604176F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{FRISTON2016862,
	title = "Active inference and learning",
	journal = "Neuroscience \& Biobehavioral Reviews",
	volume = "68",
	pages = "862 - 879",
	year = "2016",
	issn = "0149-7634",
	doi = "https://doi.org/10.1016/j.neubiorev.2016.06.022",
	author = "Karl Friston and Thomas FitzGerald and Francesco Rigoli and Philipp Schwartenbeck and John O Doherty and Giovanni Pezzulo",
	keywords = "Active inference, Habit learning, Bayesian inference, Goal-directed, Free energy, Information gain, Bayesian surprise, Epistemic value, Exploration, Exploitation",
}

@article{AI_TUTO,
title = {Active inference on discrete state-spaces: A synthesis},
journal = {Journal of Mathematical Psychology},
volume = {99},
pages = {102447},
year = {2020},
issn = {0022-2496},
doi = {https://doi.org/10.1016/j.jmp.2020.102447},
url = {https://www.sciencedirect.com/science/article/pii/S0022249620300857},
author = {Lancelot {Da Costa} and Thomas Parr and Noor Sajid and Sebastijan Veselic and Victorita Neacsu and Karl Friston},
keywords = {Active inference, Free energy principle, Process theory, Variational Bayesian inference, Markov decision process, Mathematical review},
abstract = {Active inference is a normative principle underwriting perception, action, planning, decision-making and learning in biological or artificial agents. From its inception, its associated process theory has grown to incorporate complex generative models, enabling simulation of a wide range of complex behaviours. Due to successive developments in active inference, it is often difficult to see how its underlying principle relates to process theories and practical implementation. In this paper, we try to bridge this gap by providing a complete mathematical synthesis of active inference on discrete state-space models. This technical summary provides an overview of the theory, derives neuronal dynamics from first principles and relates this dynamics to biological processes. Furthermore, this paper provides a fundamental building block needed to understand active inference for mixed generative models; allowing continuous sensations to inform discrete representations. This paper may be used as follows: to guide research towards outstanding challenges, a practical guide on how to implement active inference to simulate experimental behaviour, or a pointer towards various in-silico neurophysiological responses that may be used to make empirical predictions.}
}

@article{bayes_surprise,
title = "Bayesian surprise attracts human attention",
journal = "Vision Research",
volume = "49",
number = "10",
pages = "1295 - 1306",
year = "2009",
note = "Visual Attention: Psychophysics, electrophysiology and neuroimaging",
issn = "0042-6989",
doi = "https://doi.org/10.1016/j.visres.2008.09.007",
url = "http://www.sciencedirect.com/science/article/pii/S0042698908004380",
author = "Laurent Itti and Pierre Baldi",
keywords = "Attention, Surprise, Bayes theorem, Information theory, Eye movements, Natural vision, Free viewing, Saliency, Novelty",
abstract = "We propose a formal Bayesian definition of surprise to capture subjective aspects of sensory information. Surprise measures how data affects an observer, in terms of differences between posterior and prior beliefs about the world. Only data observations which substantially affect the observer’s beliefs yield surprise, irrespectively of how rare or informative in Shannon’s sense these observations are. We test the framework by quantifying the extent to which humans may orient attention and gaze towards surprising events or items while watching television. To this end, we implement a simple computational model where a low-level, sensory form of surprise is computed by simple simulated early visual neurons. Bayesian surprise is a strong attractor of human attention, with 72% of all gaze shifts directed towards locations more surprising than the average, a figure rising to 84% when focusing the analysis onto regions simultaneously selected by all observers. The proposed theory of surprise is applicable across different spatio-temporal scales, modalities, and levels of abstraction."
}

@article {curiosity,
	author = {Schwartenbeck, Philipp and Passecker, Johannes and Hauser, Tobias U and FitzGerald, Thomas H B and Kronbichler, Martin and Friston, Karl},
	title = {Computational mechanisms of curiosity and goal-directed exploration},
	elocation-id = {411272},
	year = {2018},
	doi = {10.1101/411272},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Successful behaviour depends on the right balance between maximising reward and soliciting information about the world. Here, we show how different types of information-gain emerge when casting behaviour as surprise minimisation. We present two distinct mechanisms for goal-directed exploration that express separable profiles of active sampling to reduce uncertainty. {\textquoteleft}Hidden state{\textquoteright} exploration motivates agents to sample unambiguous observations to accurately infer the (hidden) state of the world. Conversely, {\textquoteleft}model parameter{\textquoteright} exploration, compels agents to sample outcomes associated with high uncertainty, if they are informative for their representation of the task structure. We illustrate the emergence of these types of information-gain, termed active inference and active learning, and show how these forms of exploration induce distinct patterns of {\textquoteleft}Bayes-optimal{\textquoteright} behaviour. Our findings provide a computational framework to understand how distinct levels of uncertainty induce different modes of information-gain in decision-making.},
	URL = {https://www.biorxiv.org/content/early/2018/09/07/411272},
	eprint = {https://www.biorxiv.org/content/early/2018/09/07/411272.full.pdf},
	journal = {bioRxiv}
}

@ARTICLE{dopamine,
AUTHOR={FitzGerald, Thomas H. B. and Dolan, Raymond J. and Friston, Karl},
TITLE={Dopamine, reward learning, and active inference},      
JOURNAL={Frontiers in Computational Neuroscience},
VOLUME={9},
PAGES={136},
YEAR={2015},
URL={https://www.frontiersin.org/article/10.3389/fncom.2015.00136},
DOI={10.3389/fncom.2015.00136},
ISSN={1662-5188},   
ABSTRACT={Temporal difference learning models propose phasic dopamine signaling encodes reward prediction errors that drive learning. This is supported by studies where optogenetic stimulation of dopamine neurons can stand in lieu of actual reward. Nevertheless, a large body of data also shows that dopamine is not necessary for learning, and that dopamine depletion primarily affects task performance. We offer a resolution to this paradox based on an hypothesis that dopamine encodes the precision of beliefs about alternative actions, and thus controls the outcome-sensitivity of behavior. We extend an active inference scheme for solving Markov decision processes to include learning, and show that simulated dopamine dynamics strongly resemble those actually observed during instrumental conditioning. Furthermore, simulated dopamine depletion impairs performance but spares learning, while simulated excitation of dopamine neurons drives reward learning, through aberrant inference about outcome states. Our formal approach provides a novel and parsimonious reconciliation of apparently divergent experimental findings.}
}

@article{PAI,
title = "Planning as inference",
journal = "Trends in Cognitive Sciences",
volume = "16",
number = "10",
pages = "485 - 488",
year = "2012",
issn = "1364-6613",
doi = "https://doi.org/10.1016/j.tics.2012.08.006",
author = "Matthew Botvinick and Marc Toussaint",
}

@article{pezzato2020active,
  author    = {Corrado Pezzato and
               Carlos Hern{\'{a}}ndez Corbato and
               Martijn Wisse},
  title     = {Active Inference and Behavior Trees for Reactive Action Planning and
               Execution in Robotics},
  journal   = {CoRR},
  volume    = {abs/2011.09756},
  year      = {2020},
  url       = {https://arxiv.org/abs/2011.09756},
  eprinttype = {arXiv},
  eprint    = {2011.09756},
  timestamp = {Wed, 25 Nov 2020 16:34:14 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2011-09756.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{catal2020learning,
  abstract     = {{Active inference is a process theory of the brain that states that all living organisms infer actions in order to minimize their (expected) free energy. However, current experiments are limited to predefined, often discrete, state spaces. In this paper we use recent advances in deep learning to learn the state space and approximate the necessary probability distributions to engage in active inference.}},
  author       = {{Catal, Ozan and Verbelen, Tim and Nauta, Johannes and De Boom, Cedric and Dhoedt, Bart}},
  booktitle    = {{ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}},
  isbn         = {{9781509066315}},
  issn         = {{2379-190X}},
  keywords     = {{FREE-ENERGY PRINCIPLE,active inference,deep learning,perception,planning}},
  language     = {{eng}},
  location     = {{Barcelona, Spain}},
  pages        = {{3952--3956}},
  publisher    = {{IEEE}},
  title        = {{Learning perception and planning with deep active inference}},
  url          = {{http://dx.doi.org/10.1109/ICASSP40776.2020.9054364}},
  year         = {{2020}},
}

@inproceedings{sancaktar2020endtoend,
  author    = {Cansu Sancaktar and
               Marcel A. J. van Gerven and
               Pablo Lanillos},
  title     = {End-to-End Pixel-Based Deep Active Inference for Body Perception and
               Action},
  booktitle = {Joint {IEEE} 10th International Conference on Development and Learning
               and Epigenetic Robotics, ICDL-EpiRob 2020, Valparaiso, Chile, October
               26-30, 2020},
  pages     = {1--8},
  publisher = {{IEEE}},
  year      = {2020},
  url       = {https://doi.org/10.1109/ICDL-EpiRob48136.2020.9278105},
  doi       = {10.1109/ICDL-EpiRob48136.2020.9278105},
  timestamp = {Mon, 08 Feb 2021 13:53:48 +0100},
  biburl    = {https://dblp.org/rec/conf/icdl-epirob/SancaktarGL20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{CULLEN2018809,
title = "{Active Inference in OpenAI Gym: A Paradigm for Computational Investigations Into Psychiatric Illness}",
journal = "Biological Psychiatry: Cognitive Neuroscience and Neuroimaging",
volume = "3",
number = "9",
pages = "809 - 818",
year = "2018",
note = "Computational Methods and Modeling in Psychiatry",
issn = "2451-9022",
doi = "https://doi.org/10.1016/j.bpsc.2018.06.010",
url = "http://www.sciencedirect.com/science/article/pii/S2451902218301617",
author = "Maell Cullen and Ben Davey and Karl J. Friston and Rosalyn J. Moran",
keywords = "Active inference, Computational phenotyping, Computational psychiatry, Free energy principle, Game-based imaging biomarkers, Markov decision process",
abstract = "Background
Artificial intelligence has recently attained humanlike performance in a number of gamelike domains. These advances have been spurred by brain-inspired architectures and algorithms such as hierarchical filtering and reinforcement learning. OpenAI Gym is an open-source platform in which to train, test, and benchmark algorithms—it provides a range of tasks, including those of classic arcade games such as Doom. Here we describe how the platform might be used as a simulation, test, and diagnostic paradigm for psychiatric conditions.
Methods
To illustrate how active inference models of game play could be used to test mechanistic and algorithmic properties of psychiatric disorders, we provide two exemplar analyses. The first speaks to the impact of aging on cognition, examining game-play behaviors in a model of aging in which we compared age-dependent changes of younger (n = 9, 22 ± 1 years of age) and older (n = 7, 56 ± 5 years of age) adult players. The second is an illustration of a putative feature of anhedonia in which we simulated diminished sensitivity to reward.
Results
These simulations demonstrate how active inference can be used to test predicted changes in both neurobiology and beliefs in psychiatric cohorts. We show that, as well as behavioral measures, putative neural correlates of active inference can be simulated, and hypothesized (model-based) differences in local field potentials and blood oxygen level–dependent responses can be produced.
Conclusions
We show that active inference, through epistemic and value-based goals, enables simulated subjects to actively develop detailed representations of gaming environments, and we demonstrate the use of a principled algorithmic and neurobiological framework for testing hypotheses in psychiatric illness."
}

@misc{cart_pole,
	Title = {Combining Active Inference and Hierarchical Predictive Coding: A Tutorial Introduction and Case Study},
	Author = {Millidge, Beren},
	DOI = {10.31234/osf.io/kf6wc},
	Abstract = {&lt;p&gt;This paper combines the active inference formulation of action (Friston, 2009) with hierarchical predictive coding models (Friston, 2003) to provide a proof-of-concept implementation of an active inference agent able to solve a common reinforcement learning baseline -- the cart-pole environment in OpenAI gym. It demonstrates empirically that predictive coding and active inference approaches can be successfully scaled up to tasks more challenging than the mountain car (Friston 2009, 2012). We show that hierarchical predictive coding models can be learned from scratch during the task, and can successfully drive action selection via active inference. To our knowledge, it is the first implemented active inference agent to combine active inference with a hierarchical predictive coding perceptual model. We also provide a tutorial walk-through of the free-energy principle, hierarchical predictive coding, and active inference, including an in-depth derivation of our agent.&lt;/p&gt;},
	Publisher = {PsyArXiv},
	Year = {2019},
	URL = {https://doi.org/10.31234/osf.io/kf6wc},
}

@Article{VI_TUTO,
author={Fox, Charles W.
and Roberts, Stephen J.},
title={{A tutorial on variational Bayesian inference}},
journal={Artificial Intelligence Review},
year={2012},
month={Aug},
day={01},
volume={38},
number={2},
pages={85-95},
abstract={This tutorial describes the mean-field variational Bayesian approximation to inference in graphical models, using modern machine learning terminology rather than statistical physics concepts. It begins by seeking to find an approximate mean-field distribution close to the target joint in the KL-divergence sense. It then derives local node updates and reviews the recent Variational Message Passing framework.},
issn={1573-7462},
doi={10.1007/s10462-011-9236-8},
url={https://doi.org/10.1007/s10462-011-9236-8}
}

@Article{VMP_TUTO,
author = { John Winn and Christopher Bishop },
title = {Variational Message Passing},
journal = {Journal of Machine Learning Research},
publisher = {Microtome publishing},
year = 2005,
volume = {6},
pages = {661-694}
}

@ARTICLE{FFG_TUTO,
  author={G. D. {Forney}},
  journal={IEEE Transactions on Information Theory}, 
  title={Codes on graphs: normal realizations}, 
  year={2001},
  volume={47},
  number={2},
  pages={520-548}
}

@article{Simul_AI,
  author    = {Thijs van de Laar and
               Bert de Vries},
  title     = {Simulating Active Inference Processes by Message Passing},
  journal   = {Front. Robotics and {AI}},
  volume    = {2019},
  year      = {2019},
  url       = {https://doi.org/10.3389/frobt.2019.00020},
  doi       = {10.3389/frobt.2019.00020},
  timestamp = {Tue, 16 Apr 2019 16:32:33 +0200},
  biburl    = {https://dblp.org/rec/journals/firai/LaarV19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/ijar/CoxLV19,
  author    = {Marco Cox and
               Thijs van de Laar and
               Bert de Vries},
  title     = {A factor graph approach to automated design of {Bayesian} signal processing
               algorithms},
  journal   = {Int. J. Approx. Reason.},
  volume    = {104},
  pages     = {185--204},
  year      = {2019},
  url       = {https://doi.org/10.1016/j.ijar.2018.11.002},
  doi       = {10.1016/j.ijar.2018.11.002},
  timestamp = {Fri, 21 Feb 2020 13:12:23 +0100},
  biburl    = {https://dblp.org/rec/journals/ijar/CoxLV19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{6145622,
  author={C. B. {Browne} and E. {Powley} and D. {Whitehouse} and S. M. {Lucas} and P. I. {Cowling} and P. {Rohlfshagen} and S. {Tavener} and D. {Perez} and S. {Samothrakis} and S. {Colton}},
  journal={IEEE Transactions on Computational Intelligence and AI in Games}, 
  title={A Survey of Monte Carlo Tree Search Methods}, 
  year={2012},
  volume={4},
  number={1},
  pages={1-43}
}

@article{MuZero,
  title={{Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model}},
  author={Julian Schrittwieser and Ioannis Antonoglou and Thomas Hubert and Karen Simonyan and Laurent Sifre and Simon Schmitt and Arthur Guez and Edward Lockhart and Demis Hassabis and Thore Graepel and Timothy P. Lillicrap and David Silver},
  journal={ArXiv},
  year={2019},
  volume={abs/1911.08265}
}

@misc{dacosta2020relationship,
      title={The relationship between dynamic programming and active inference: the discrete, finite-horizon case}, 
      author={Lancelot Da Costa and Noor Sajid and Thomas Parr and Karl Friston and Ryan Smith},
      year={2020},
      eprint={2009.08111},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@ARTICLE{10.3389/fncom.2015.00136,
AUTHOR={FitzGerald, Thomas H. B. and Dolan, Raymond J. and Friston, Karl},   
TITLE={Dopamine, reward learning, and active inference},      
JOURNAL={Frontiers in Computational Neuroscience},      
VOLUME={9},      
PAGES={136},     
YEAR={2015},      
URL={https://www.frontiersin.org/article/10.3389/fncom.2015.00136},       
DOI={10.3389/fncom.2015.00136},      
ISSN={1662-5188},   
ABSTRACT={Temporal difference learning models propose phasic dopamine signaling encodes reward prediction errors that drive learning. This is supported by studies where optogenetic stimulation of dopamine neurons can stand in lieu of actual reward. Nevertheless, a large body of data also shows that dopamine is not necessary for learning, and that dopamine depletion primarily affects task performance. We offer a resolution to this paradox based on an hypothesis that dopamine encodes the precision of beliefs about alternative actions, and thus controls the outcome-sensitivity of behavior. We extend an active inference scheme for solving Markov decision processes to include learning, and show that simulated dopamine dynamics strongly resemble those actually observed during instrumental conditioning. Furthermore, simulated dopamine depletion impairs performance but spares learning, while simulated excitation of dopamine neurons drives reward learning, through aberrant inference about outcome states. Our formal approach provides a novel and parsimonious reconciliation of apparently divergent experimental findings.}
}

@ARTICLE{AI_DISCRET,
AUTHOR={Friston, Karl and Schwartenbeck, Philipp and Fitzgerald, Thomas and Moutoussis, Michael and Behrens, Tim and Dolan, Raymond},   
TITLE={The anatomy of choice: active inference and agency},      
JOURNAL={Frontiers in Human Neuroscience},      
VOLUME={7},     
PAGES={598},    
YEAR={2013},      
URL={https://www.frontiersin.org/article/10.3389/fnhum.2013.00598},       
DOI={10.3389/fnhum.2013.00598},      
ISSN={1662-5161},   
ABSTRACT={This paper considers agency in the setting of embodied or active inference. In brief, we associate a sense of agency with prior beliefs about action and ask what sorts of beliefs underlie optimal behavior. In particular, we consider prior beliefs that action minimizes the Kullback–Leibler (KL) divergence between desired states and attainable states in the future. This allows one to formulate bounded rationality as approximate Bayesian inference that optimizes a free energy bound on model evidence. We show that constructs like expected utility, exploration bonuses, softmax choice rules and optimism bias emerge as natural consequences of this formulation. Previous accounts of active inference have focused on predictive coding and Bayesian filtering schemes for minimizing free energy. Here, we consider variational Bayes as an alternative scheme that provides formal constraints on the computational anatomy of inference and action—constraints that are remarkably consistent with neuroanatomy. Furthermore, this scheme contextualizes optimal decision theory and economic (utilitarian) formulations as pure inference problems. For example, expected utility theory emerges as a special case of free energy minimization, where the sensitivity or inverse temperature (of softmax functions and quantal response equilibria) has a unique and Bayes-optimal solution—that minimizes free energy. This sensitivity corresponds to the precision of beliefs about behavior, such that attainable goals are afforded a higher precision or confidence. In turn, this means that optimal behavior entails a representation of confidence about outcomes that are under an agent's control.}
}

@article {Parr304782,
	author = {Parr, Thomas and Friston, Karl J},
	title = {Generalised free energy and active inference: can the future cause the past?},
	elocation-id = {304782},
	year = {2018},
	doi = {10.1101/304782},
	publisher = {Cold Spring Harbor Laboratory},
	URL = {https://www.biorxiv.org/content/early/2018/04/23/304782},
	eprint = {https://www.biorxiv.org/content/early/2018/04/23/304782.full.pdf},
	journal = {bioRxiv}
}

@article{millidge2020expected,
  author    = {Beren Millidge and
               Alexander Tschantz and
               Christopher L. Buckley},
  title     = {Whence the Expected Free Energy?},
  journal   = {Neural Comput.},
  volume    = {33},
  number    = {2},
  pages     = {447--482},
  year      = {2021},
  url       = {https://doi.org/10.1162/neco\_a\_01354},
  doi       = {10.1162/neco\_a\_01354},
  timestamp = {Tue, 01 Jun 2021 09:59:43 +0200},
  biburl    = {https://dblp.org/rec/journals/neco/MillidgeTB21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DeepAI,
title = "Deep active inference as variational policy gradients",
journal = "Journal of Mathematical Psychology",
volume = "96",
pages = "102348",
year = "2020",
issn = "0022-2496",
doi = "https://doi.org/10.1016/j.jmp.2020.102348",
url = "http://www.sciencedirect.com/science/article/pii/S0022249620300298",
author = "Beren Millidge",
keywords = "Active inference, Predictive processing, Neural networks, Policy gradients, Reinforcement learning",
}

@article{PixelBasedAI,
  title={End-to-End Pixel-Based Deep Active Inference for Body Perception and Action},
  author={Cansu Sancaktar and Pablo Lanillos},
  journal={ArXiv},
  year={2020},
  volume={abs/2001.05847}
}

@article{DBLP:journals/corr/abs-1801-01290,
  author    = {Tuomas Haarnoja and
               Aurick Zhou and
               Pieter Abbeel and
               Sergey Levine},
  title     = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning
               with a Stochastic Actor},
  journal   = {CoRR},
  volume    = {abs/1801.01290},
  year      = {2018},
  url       = {http://arxiv.org/abs/1801.01290},
  archivePrefix = {arXiv},
  eprint    = {1801.01290},
  timestamp = {Mon, 13 Aug 2018 16:48:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1801-01290.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DeepRL,
  author    = {Volodymyr Mnih and
               Koray Kavukcuoglu and
               David Silver and
               Alex Graves and
               Ioannis Antonoglou and
               Daan Wierstra and
               Martin A. Riedmiller},
  title     = {{Playing Atari with Deep Reinforcement Learning}},
  journal   = {CoRR},
  volume    = {abs/1312.5602},
  year      = {2013},
  url       = {http://arxiv.org/abs/1312.5602},
  eprinttype = {arXiv},
  eprint    = {1312.5602},
  timestamp = {Mon, 13 Aug 2018 16:47:42 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/MnihKSGAWR13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DDQN,
  author    = {Hado van Hasselt and
               Arthur Guez and
               David Silver},
  title     = {Deep Reinforcement Learning with Double Q-learning},
  journal   = {CoRR},
  volume    = {abs/1509.06461},
  year      = {2015},
  url       = {http://arxiv.org/abs/1509.06461},
  eprinttype = {arXiv},
  eprint    = {1509.06461},
  timestamp = {Mon, 13 Aug 2018 16:47:32 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HasseltGS15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lample2016playing,
  author    = {Guillaume Lample and
               Devendra Singh Chaplot},
  editor    = {Satinder P. Singh and
               Shaul Markovitch},
  title     = {Playing {FPS} Games with Deep Reinforcement Learning},
  booktitle = {Proceedings of the Thirty-First {AAAI} Conference on Artificial Intelligence,
               February 4-9, 2017, San Francisco, California, {USA}},
  pages     = {2140--2146},
  publisher = {{AAAI} Press},
  year      = {2017},
  url       = {http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14456},
  timestamp = {Wed, 10 Feb 2021 08:43:37 +0100},
  biburl    = {https://dblp.org/rec/conf/aaai/LampleC17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{BOTVINICK2019408,
title = "Reinforcement Learning, Fast and Slow",
journal = "Trends in Cognitive Sciences",
volume = "23",
number = "5",
pages = "408 - 422",
year = "2019",
issn = "1364-6613",
doi = "https://doi.org/10.1016/j.tics.2019.02.006",
url = "http://www.sciencedirect.com/science/article/pii/S1364661319300610",
author = "Matthew Botvinick and Sam Ritter and Jane X. Wang and Zeb Kurth-Nelson and Charles Blundell and Demis Hassabis",
abstract = "Deep reinforcement learning (RL) methods have driven impressive advances in artificial intelligence in recent years, exceeding human performance in domains ranging from Atari to Go to no-limit poker. This progress has drawn the attention of cognitive scientists interested in understanding human learning. However, the concern has been raised that deep RL may be too sample-inefficient – that is, it may simply be too slow – to provide a plausible model of how humans learn. In the present review, we counter this critique by describing recently developed techniques that allow deep RL to operate more nimbly, solving problems much more quickly than previous methods. Although these techniques were developed in an AI context, we propose that they may have rich implications for psychology and neuroscience. A key insight, arising from these AI methods, concerns the fundamental connection between fast RL and slower, more incremental forms of learning."
}




























@article{mnist,
  added-at = {2010-06-28T21:16:30.000+0200},
  author = {LeCun, Yann and Cortes, Corinna},
  biburl = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/mhwombat},
  groups = {public},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  interhash = {21b9d0558bd66279df9452562df6e6f3},
  intrahash = {935bad99fa1f65e03c25b315aa3c1032},
  keywords = {MSc _checked character_recognition mnist network neural},
  lastchecked = {2016-01-14 14:24:11},
  timestamp = {2016-07-12T19:25:30.000+0200},
  title = {{MNIST} handwritten digit database},
  url = {http://yann.lecun.com/exdb/mnist/},
  username = {mhwombat},
  year = 2010
}

@misc{levine2018reinforcement,
      title={Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review}, 
      author={Sergey Levine},
      year={2018},
      eprint={1805.00909},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{SVI1,
  author    = {Wim Wiegerinck},
  editor    = {Craig Boutilier and
               Mois{\'{e}}s Goldszmidt},
  title     = {Variational Approximations between Mean Field Theory and the Junction
               Tree Algorithm},
  booktitle = {{UAI} '00: Proceedings of the 16th Conference in Uncertainty in Artificial
               Intelligence, Stanford University, Stanford, California, USA, June
               30 - July 3, 2000},
  pages     = {626--633},
  publisher = {Morgan Kaufmann},
  year      = {2000},
  url       = {https://dslpitt.org/uai/displayArticleDetails.jsp?mmnu=1\&smnu=2\&article\_id=73\&proceeding\_id=16},
  timestamp = {Wed, 06 May 2015 15:02:56 +0200},
  biburl    = {https://dblp.org/rec/conf/uai/Wiegerinck00.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{SVI2,
  author    = {Eric P. Xing and
               Michael I. Jordan and
               Stuart J. Russell},
  title     = {A Generalized Mean Field Algorithm for Variational Inference in Exponential
               Families},
  journal   = {CoRR},
  volume    = {abs/1212.2512},
  year      = {2012},
  url       = {http://arxiv.org/abs/1212.2512},
  archivePrefix = {arXiv},
  eprint    = {1212.2512},
  timestamp = {Mon, 13 Aug 2018 16:46:42 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1212-2512.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{910572,
  author={F. R. {Kschischang} and B. J. {Frey} and H. -. {Loeliger}},
  journal={IEEE Transactions on Information Theory}, 
  title={Factor graphs and the sum-product algorithm}, 
  year={2001},
  volume={47},
  number={2},
  pages={498-519}
}
  
@Article{Berridge2007,
author={Berridge, Kent C.},
title={The debate over dopamine's role in reward: the case for incentive salience},
journal={Psychopharmacology},
year={2007},
month={Apr},
day={01},
volume={191},
number={3},
pages={391-431},
abstract={Debate continues over the precise causal contribution made by mesolimbic dopamine systems to reward. There are three competing explanatory categories: `liking', learning, and `wanting'. Does dopamine mostly mediate the hedonic impact of reward (`liking')? Does it instead mediate learned predictions of future reward, prediction error teaching signals and stamp in associative links (learning)? Or does dopamine motivate the pursuit of rewards by attributing incentive salience to reward-related stimuli (`wanting')? Each hypothesis is evaluated here, and it is suggested that the incentive salience or `wanting' hypothesis of dopamine function may be consistent with more evidence than either learning or `liking'. In brief, recent evidence indicates that dopamine is neither necessary nor sufficient to mediate changes in hedonic `liking' for sensory pleasures. Other recent evidence indicates that dopamine is not needed for new learning, and not sufficient to directly mediate learning by causing teaching or prediction signals. By contrast, growing evidence indicates that dopamine does contribute causally to incentive salience. Dopamine appears necessary for normal `wanting', and dopamine activation can be sufficient to enhance cue-triggered incentive salience. Drugs of abuse that promote dopamine signals short circuit and sensitize dynamic mesolimbic mechanisms that evolved to attribute incentive salience to rewards. Such drugs interact with incentive salience integrations of Pavlovian associative information with physiological state signals. That interaction sets the stage to cause compulsive `wanting' in addiction, but also provides opportunities for experiments to disentangle `wanting', `liking', and learning hypotheses. Results from studies that exploited those opportunities are described here.},
issn={1432-2072},
doi={10.1007/s00213-006-0578-x},
url={https://doi.org/10.1007/s00213-006-0578-x}
}

@article {Schultz1593,
	author = {Schultz, Wolfram and Dayan, Peter and Montague, P. Read},
	title = {A Neural Substrate of Prediction and Reward},
	volume = {275},
	number = {5306},
	pages = {1593--1599},
	year = {1997},
	doi = {10.1126/science.275.5306.1593},
	publisher = {American Association for the Advancement of Science},
	abstract = {The capacity to predict future events permits a creature to detect, model, and manipulate the causal structure of its interactions with its environment. Behavioral experiments suggest that learning is driven by changes in the expectations about future salient events such as rewards and punishments. Physiological work has recently complemented these studies by identifying dopaminergic neurons in the primate whose fluctuating output apparently signals changes or errors in the predictions of future salient and rewarding events. Taken together, these findings can be understood through quantitative theories of adaptive optimizing control.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/275/5306/1593},
	eprint = {https://science.sciencemag.org/content/275/5306/1593.full.pdf},
	journal = {Science}
}

@misc{koller2009probabilistic,
  title={Probabilistic Graphical Models, Massachusetts},
  author={Koller, D and Friedman, N},
  year={2009},
  publisher={MIT Press USA}
}

@article{sophisticated,
    author = {Friston, Karl and Da Costa, Lancelot and Hafner, Danijar and Hesp, Casper and Parr, Thomas},
    title = "{Sophisticated Inference}",
    journal = {Neural Computation},
    volume = {33},
    number = {3},
    pages = {713-763},
    year = {2021},
    month = {03},
    abstract = "{Active inference offers a first principle account of sentient behavior, from which special and important cases—for example, reinforcement learning, active learning, Bayes optimal inference, Bayes optimal design—can be derived. Active inference finesses the exploitation-exploration dilemma in relation to prior preferences by placing information gain on the same footing as reward or value. In brief, active inference replaces value functions with functionals of (Bayesian) beliefs, in the form of an expected (variational) free energy. In this letter, we consider a sophisticated kind of active inference using a recursive form of expected free energy. Sophistication describes the degree to which an agent has beliefs about beliefs. We consider agents with beliefs about the counterfactual consequences of action for states of affairs and beliefs about those latent states. In other words, we move from simply considering beliefs about “what would happen if I did that” to “what I would believe about what would happen if I did that.” The recursive form of the free energy functional effectively implements a deep tree search over actions and outcomes in the future. Crucially, this search is over sequences of belief states as opposed to states per se. We illustrate the competence of this scheme using numerical simulations of deep decision problems.}",
    issn = {0899-7667},
    doi = {10.1162/neco_a_01351},
    url = {https://doi.org/10.1162/neco\_a\_01351},
    eprint = {https://direct.mit.edu/neco/article-pdf/33/3/713/1889421/neco\_a\_01351.pdf},
}

@Article{BP_and_DC,
author={Yedidia, Jonathan S.},
title={Message-Passing Algorithms for Inference and Optimization},
journal={Journal of Statistical Physics},
year={2011},
month={Nov},
day={01},
volume={145},
number={4},
pages={860-890},
abstract={Message-passing algorithms can solve a wide variety of optimization, inference, and constraint satisfaction problems. The algorithms operate on factor graphs that visually represent and specify the structure of the problems. After describing some of their applications, I survey the family of belief propagation (BP) algorithms, beginning with a detailed description of the min-sum algorithm and its exactness on tree factor graphs, and then turning to a variety of more sophisticated BP algorithms, including free-energy based BP algorithms, ``splitting'' BP algorithms that generalize ``tree-reweighted'' BP, and the various BP algorithms that have been proposed to deal with problems with continuous variables.},
issn={1572-9613},
doi={10.1007/s10955-011-0384-7},
url={https://doi.org/10.1007/s10955-011-0384-7}
}

@Article{believe,
author = {Friston, Karl J. and Parr, Thomas and de Vries, Bert},
title = {The graphical brain: Belief propagation and active inference},
journal = {Network Neuroscience},
volume = {1},
number = {4},
pages = {381-414},
year = {2017},
doi = {10.1162/NETN\_a\_00018},
URL = {https://doi.org/10.1162/NETN_a_00018},
eprint = {https://doi.org/10.1162/NETN_a_00018},
abstract = { This paper considers functional integration in the brain from a computational perspective. We ask what sort of neuronal message passing is mandated by active inference—and what implications this has for context-sensitive connectivity at microscopic and macroscopic levels. In particular, we formulate neuronal processing as belief propagation under deep generative models. Crucially, these models can entertain both discrete and continuous states, leading to distinct schemes for belief updating that play out on the same (neuronal) architecture. Technically, we use Forney (normal) factor graphs to elucidate the requisite message passing in terms of its form and scheduling. To accommodate mixed generative models (of discrete and continuous states), one also has to consider link nodes or factors that enable discrete and continuous representations to talk to each other. When mapping the implicit computational architecture onto neuronal connectivity, several interesting features emerge. For example, Bayesian model averaging and comparison, which link discrete and continuous states, may be implemented in thalamocortical loops. These and other considerations speak to a computational connectome that is inherently state dependent and self-organizing in ways that yield to a principled (variational) account. We conclude with simulations of reading that illustrate the implicit neuronal message passing, with a special focus on how discrete (semantic) representations inform, and are informed by, continuous (visual) sampling of the sensorium.}
}

@Article{Friston2010,
author={Friston, Karl},
title={The free-energy principle: a unified brain theory?},
journal={Nature Reviews Neuroscience},
year={2010},
month={Feb},
day={01},
volume={11},
number={2},
pages={127-138},
abstract={Adaptive agents must occupy a limited repertoire of states and therefore minimize the long-term average of surprise associated with sensory exchanges with the world. Minimizing surprise enables them to resist a natural tendency to disorder.Surprise rests on predictions about sensations, which depend on an internal generative model of the world. Although surprise cannot be measured directly, a free-energy bound on surprise can be, suggesting that agents minimize free energy by changing their predictions (perception) or by changing the predicted sensory inputs (action).Perception optimizes predictions by minimizing free energy with respect to synaptic activity (perceptual inference), efficacy (learning and memory) and gain (attention and salience). This furnishes Bayes-optimal (probabilistic) representations of what caused sensations (providing a link to the Bayesian brain hypothesis).Bayes-optimal perception is mathematically equivalent to predictive coding and maximizing the mutual information between sensations and the representations of their causes. This is a probabilistic generalization of the principle of efficient coding (the infomax principle) or the minimum-redundancy principle.Learning under the free-energy principle can be formulated in terms of optimizing the connection strengths in hierarchical models of the sensorium. This rests on associative plasticity to encode causal regularities and appeals to the same synaptic mechanisms as those underlying cell assembly formation.Action under the free-energy principle reduces to suppressing sensory prediction errors that depend on predicted (expected or desired) movement trajectories. This provides a simple account of motor control, in which action is enslaved by perceptual (proprioceptive) predictions.Perceptual predictions rest on prior expectations about the trajectory or movement through the agent's state space. These priors can be acquired (as empirical priors during hierarchical inference) or they can be innate (epigenetic) and therefore subject to selective pressure.Predicted motion or state transitions realized by action correspond to policies in optimal control theory and reinforcement learning. In this context, value is inversely proportional to surprise (and implicitly free energy), and rewards correspond to innate priors that constrain policies.},
issn={1471-0048},
doi={10.1038/nrn2787},
url={https://doi.org/10.1038/nrn2787}
}

@article{BUCKLEY201755,
title = "The free energy principle for action and perception: A mathematical review",
journal = "Journal of Mathematical Psychology",
volume = "81",
pages = "55 - 79",
year = "2017",
issn = "0022-2496",
doi = "https://doi.org/10.1016/j.jmp.2017.09.004",
author = "Christopher L. Buckley and Chang Sub Kim and Simon McGregor and Anil K. Seth",
keywords = "Free energy principle, Perception, Action, Inference, Bayesian brain, Agent-based model",
}

@ARTICLE{active_vision,  author={D. {Ognibene} and G. {Baldassare}},  journal={IEEE Transactions on Autonomous Mental Development},   title={Ecological Active Vision: Four Bioinspired Principles to Integrate Bottom–Up and Adaptive Top–Down Attention Tested With a Simple Camera-Arm Robot},   year={2015},  volume={7},  number={1},  pages={3-25},}

@article{bellman1954,
author = "Bellman, Richard",
fjournal = "Bulletin of the American Mathematical Society",
journal = "Bull. Amer. Math. Soc.",
month = "11",
number = "6",
pages = "503--515",
publisher = "American Mathematical Society",
title = "The theory of dynamic programming",
url = "https://projecteuclid.org:443/euclid.bams/1183519147",
volume = "60",
year = "1954"
}

@article{10000025057,
author="Watkins, C. J. C. H.",
title="Learning from delayed rewards",
journal="PhD thesis, Cambridge University",
ISSN="",
publisher="",
year="1989",
month="",
volume="",
number="",
pages="",
URL="https://ci.nii.ac.jp/naid/10000025057/en/",
DOI="",
}

@incollection{NIPS2010_3964,
title = {Double {Q}-learning},
author = {Hado V. Hasselt},
booktitle = {Advances in Neural Information Processing Systems 23},
editor = {J. D. Lafferty and C. K. I. Williams and J. Shawe-Taylor and R. S. Zemel and A. Culotta},
pages = {2613--2621},
year = {2010},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/3964-double-q-learning.pdf}
}

@Article{adaptive_coding,
author={Duncan, John},
title={An adaptive coding model of neural function in prefrontal cortex},
journal={Nature Reviews Neuroscience},
year={2001},
month={Nov},
day={01},
volume={2},
number={11},
pages={820-829},
issn={1471-0048},
doi={10.1038/35097575},
url={https://doi.org/10.1038/35097575}
}

@inproceedings{Rish2001AnES,
  title={An empirical study of the naive {Bayes} classifier},
  author={Irina Rish},
  year={2001}
}

@inproceedings{Metsis06spamfiltering,
    author = {Vangelis Metsis and et al.},
    title = {Spam Filtering with Naive {Bayes} -- Which Naive {Bayes}?},
    booktitle = {THIRD CONFERENCE ON EMAIL AND ANTI-SPAM (CEAS},
    year = {2006},
    publisher = {}
}

@article{LDA,
author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
title = {Latent {Dirichlet} Allocation},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
month = mar,
pages = {993–1022},
numpages = {30}
}

@ARTICLE{HMM,
  author={L. {Rabiner} and B. {Juang}},
  journal={IEEE ASSP Magazine}, 
  title={An introduction to hidden {Markov} models}, 
  year={1986},
  volume={3},
  number={1},
  pages={4-16}
}

@Book{Sampling_Chapter,
author = { Christopher Bishop },
title = {Pattern Recognition and Machine Learning},
publisher = {Springer},
year = 2006,
pages = {738},
chapter = 11
}

@article{occam,
title = "Occam's Razor",
journal = "Information Processing Letters",
volume = "24",
number = "6",
pages = "377 - 380",
year = "1987",
issn = "0020-0190",
doi = "https://doi.org/10.1016/0020-0190(87)90114-1",
url = "http://www.sciencedirect.com/science/article/pii/0020019087901141",
author = "Anselm Blumer and Andrzej Ehrenfeucht and David Haussler and Manfred K. Warmuth",
keywords = "Machine learning, induction, inductive inference, Occam's Razor, methodology of science",
abstract = "We show that a polynomial learning algorithm, as defined by Valiant (1984), is obtained whenever there exists a polynomial-time method of producing, for any sequence of observations, a nearly minimum hypothesis that is consistent with these observations."
}

@inproceedings{KL_CONTROL,
author = {Rawlik, Konrad and Toussaint, Marc and Vijayakumar, Sethu},
title = {On Stochastic Optimal Control and Reinforcement Learning by Approximate Inference (Extended Abstract)},
year = {2013},
isbn = {9781577356332},
publisher = {AAAI Press},
booktitle = {Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence},
pages = {3052–3056},
numpages = {5},
location = {Beijing, China},
series = {IJCAI ’13}
}

@article{EPISTEMIC_VALUE,
author = {Karl Friston and Francesco Rigoli and Dimitri Ognibene and Christoph Mathys and Thomas Fitzgerald and Giovanni Pezzulo},
title = {Active inference and epistemic value},
journal = {Cognitive Neuroscience},
volume = {6},
number = {4},
pages = {187-214},
year  = {2015},
publisher = {Routledge},
doi = {10.1080/17588928.2015.1020053},
note ={PMID: 25689102},
URL = { https://doi.org/10.1080/17588928.2015.1020053},
eprint = { https://doi.org/10.1080/17588928.2015.1020053}
}

@Article{SC2,
author={Vinyals, Oriol
and Babuschkin, Igor
and Czarnecki, Wojciech M.
and Mathieu, Micha{\"e}l
and Dudzik, Andrew
and Chung, Junyoung
and Choi, David H.
and Powell, Richard
and Ewalds, Timo
and Georgiev, Petko
and Oh, Junhyuk
and Horgan, Dan
and Kroiss, Manuel
and Danihelka, Ivo
and Huang, Aja
and Sifre, Laurent
and Cai, Trevor
and Agapiou, John P.
and Jaderberg, Max
and Vezhnevets, Alexander S.
and Leblond, R{\'e}mi
and Pohlen, Tobias
and Dalibard, Valentin
and Budden, David
and Sulsky, Yury
and Molloy, James
and Paine, Tom L.
and Gulcehre, Caglar
and Wang, Ziyu
and Pfaff, Tobias
and Wu, Yuhuai
and Ring, Roman
and Yogatama, Dani
and W{\"u}nsch, Dario
and McKinney, Katrina
and Smith, Oliver
and Schaul, Tom
and Lillicrap, Timothy
and Kavukcuoglu, Koray
and Hassabis, Demis
and Apps, Chris
and Silver, David},
title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
journal={Nature},
year={2019},
month={Nov},
day={01},
volume={575},
number={7782},
pages={350-354},
abstract={Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1--3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8{\%} of officially ranked human players.},
issn={1476-4687},
doi={10.1038/s41586-019-1724-z},
url={https://doi.org/10.1038/s41586-019-1724-z}
}

@inproceedings{MAXQ,
  author    = {Thomas G. Dietterich},
  title     = {The {MAXQ} Method for Hierarchical Reinforcement Learning},
  booktitle = {Proceedings of the Fifteenth International Conference on Machine Learning
               {(ICML} 1998), Madison, Wisconsin, USA, July 24-27, 1998},
  pages     = {118--126},
  year      = {1998},
  crossref  = {DBLP:conf/icml/1998},
  timestamp = {Thu, 30 Jun 2011 10:34:12 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/Dietterich98.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@proceedings{DBLP:conf/icml/1998,
  editor    = {Jude W. Shavlik},
  title     = {Proceedings of the Fifteenth International Conference on Machine Learning
               {(ICML} 1998), Madison, Wisconsin, USA, July 24-27, 1998},
  publisher = {Morgan Kaufmann},
  year      = {1998},
  isbn      = {1-55860-556-8},
  timestamp = {Thu, 30 Jun 2011 10:34:12 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/1998.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{BHRL,
title = {{Bayesian} Hierarchical Reinforcement Learning},
author = {Feng Cao and Soumya Ray},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {73--81},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4752-bayesian-hierarchical-reinforcement-learning.pdf}
}

@article{FRISTON2018486,
title = "Deep temporal models and active inference",
journal = "Neuroscience \& Biobehavioral Reviews",
volume = "90",
pages = "486 - 501",
year = "2018",
issn = "0149-7634",
doi = "https://doi.org/10.1016/j.neubiorev.2018.04.004",
url = "http://www.sciencedirect.com/science/article/pii/S0149763418302525",
author = "Karl J. Friston and Richard Rosch and Thomas Parr and Cathy Price and Howard Bowman",
keywords = "Active inference, Bayesian, Hierarchical, Reading, Violation, Free energy, P300, MMN"
}

@article{Watkins1992,
	abstract = {
$$\mathcal\{Q\}$$  -learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem for     $$\mathcal\{Q\}$$  -learning based on that outlined in Watkins (1989). We show that     $$\mathcal\{Q\}$$  -learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where many     $$\mathcal\{Q\}$$   values can be changed each iteration, rather than just one.
},
	affiliation = {25b Framfield Road, N5 1UU Highbury, London, England; Centre for Cognitive Science, University of Edinburgh},
	author = {Watkins, Christopher J.C.H. and Dayan, Peter},
	copyright = {Kluwer Academic Publishers},
	doi = {10.1023/A:1022676722315},
	journal = {Machine Learning},
	keywords = {$$\mathcal\{Q\}$$  -learning; reinforcement learning; temporal differences; asynchronous dynamic programming},
	language = {English},
	number = {3-4},
	pages = {279-292},
	title = {Technical Note: Q-Learning},
	volume = {8},
	year = {1992},
}

@article{hippocampus_and_ghrelin,
author = {Kojima, Masayasu and Kangawa, Kenji},
title = {Ghrelin: Structure and Function},
journal = {Physiological Reviews},
volume = {85},
number = {2},
pages = {495-522},
year = {2005},
doi = {10.1152/physrev.00012.2004},
note ={PMID: 15788704},
URL = {https://doi.org/10.1152/physrev.00012.2004},
eprint = {https://doi.org/10.1152/physrev.00012.2004},
abstract = { Small synthetic molecules called growth hormone secretagogues (GHSs) stimulate the release of growth hormone (GH) from the pituitary. They act through the GHS-R, a G protein-coupled receptor whose ligand has only been discovered recently. Using a reverse pharmacology paradigm with a stable cell line expressing GHS-R, we purified an endogenous ligand for GHS-R from rat stomach and named it “ghrelin,” after a word root (“ghre”) in Proto-Indo-European languages meaning “grow.” Ghrelin is a peptide hormone in which the third amino acid, usually a serine but in some species a threonine, is modified by a fatty acid; this modification is essential for ghrelin's activity. The discovery of ghrelin indicates that the release of GH from the pituitary might be regulated not only by hypothalamic GH-releasing hormone, but also by ghrelin derived from the stomach. In addition, ghrelin stimulates appetite by acting on the hypothalamic arcuate nucleus, a region known to control food intake. Ghrelin is orexigenic; it is secreted from the stomach and circulates in the bloodstream under fasting conditions, indicating that it transmits a hunger signal from the periphery to the central nervous system. Taking into account all these activities, ghrelin plays important roles for maintaining GH release and energy homeostasis in vertebrates.}
}


@online{oleg,
  author = {Oleg Solopchuk},
  title = {Tutorial on Active Inference},
  year = 2018,
  url = {https://medium.com/@solopchuk/tutorial-on-active-inference-30edcf50f5dc},
  urldate = {2018-09-04}
}

@article{FRISTON2016413,
title = {Bayesian model reduction and empirical Bayes for group (DCM) studies},
journal = {NeuroImage},
volume = {128},
pages = {413-431},
year = {2016},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2015.11.015},
url = {https://www.sciencedirect.com/science/article/pii/S105381191501037X},
author = {Karl J. Friston and Vladimir Litvak and Ashwini Oswal and Adeel Razi and Klaas E. Stephan and Bernadette C.M. {van Wijk} and Gabriel Ziegler and Peter Zeidman},
keywords = {Empirical Bayes, Random effects, Fixed effects, Dynamic causal modelling, Classification, Bayesian model reduction, Hierarchical modelling},
abstract = {This technical note describes some Bayesian procedures for the analysis of group studies that use nonlinear models at the first (within-subject) level – e.g., dynamic causal models – and linear models at subsequent (between-subject) levels. Its focus is on using Bayesian model reduction to finesse the inversion of multiple models of a single dataset or a single (hierarchical or empirical Bayes) model of multiple datasets. These applications of Bayesian model reduction allow one to consider parametric random effects and make inferences about group effects very efficiently (in a few seconds). We provide the relatively straightforward theoretical background to these procedures and illustrate their application using a worked example. This example uses a simulated mismatch negativity study of schizophrenia. We illustrate the robustness of Bayesian model reduction to violations of the (commonly used) Laplace assumption in dynamic causal modelling and show how its recursive application can facilitate both classical and Bayesian inference about group differences. Finally, we consider the application of these empirical Bayesian procedures to classification and prediction.}
}

@ARTICLE{BMR,
       author = {{Friston}, Karl and {Parr}, Thomas and {Zeidman}, Peter},
        title = "{{Bayesian} model reduction}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Methodology},
         year = 2018,
        month = may,
          eid = {arXiv:1805.07092},
        pages = {arXiv:1805.07092},
archivePrefix = {arXiv},
       eprint = {1805.07092},
 primaryClass = {stat.ME},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180507092F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article {BME,
	author = {Smith, Ryan and Schwartenbeck, Philipp and Parr, Thomas and Friston, Karl J.},
	title = {An active inference approach to modeling concept learning},
	elocation-id = {633677},
	year = {2019},
	doi = {10.1101/633677},
	publisher = {Cold Spring Harbor Laboratory},
	URL = {https://www.biorxiv.org/content/early/2019/05/23/633677},
	eprint = {https://www.biorxiv.org/content/early/2019/05/23/633677.full.pdf},
	journal = {bioRxiv}
}

@article{Pang_2019,
   title={On Reinforcement Learning for Full-Length Game of StarCraft},
   volume={33},
   ISSN={2159-5399},
   url={http://dx.doi.org/10.1609/aaai.v33i01.33014691},
   DOI={10.1609/aaai.v33i01.33014691},
   journal={Proceedings of the AAAI Conference on Artificial Intelligence},
   publisher={Association for the Advancement of Artificial Intelligence (AAAI)},
   author={Pang, Zhen-Jia and Liu, Ruo-Ze and Meng, Zhou-Yu and Zhang, Yi and Yu, Yang and Lu, Tong},
   year={2019},
   month={Jul},
   pages={4691–4698}
}

@misc{VAE,
    title={Tutorial on Variational Autoencoders},
    author={Carl Doersch},
    year={2016},
    eprint={1606.05908},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@inproceedings{beta-VAE,
  author    = {Irina Higgins and
               Lo{\"{\i}}c Matthey and
               Arka Pal and
               Christopher Burgess and
               Xavier Glorot and
               Matthew Botvinick and
               Shakir Mohamed and
               Alexander Lerchner},
  title     = {beta-{VAE}: Learning Basic Visual Concepts with a Constrained Variational
               Framework},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  year      = {2017},
  crossref  = {DBLP:conf/iclr/2017},
  url       = {https://openreview.net/forum?id=Sy2fzU9gl},
  timestamp = {Thu, 25 Jul 2019 14:25:41 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/HigginsMPBGBML17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@proceedings{DBLP:conf/iclr/2017,
  title     = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/group?id=ICLR.cc/2017/conference},
  timestamp = {Thu, 04 Apr 2019 13:20:07 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/2017.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{designpatterns,
    author = {Dietmar Kühl},
    title = {Design patterns for the implementation of graph algorithms},
    booktitle = {MASTER’S THESIS, TECHNISCHE UNIVERSITÄT},
    year = {1996},
    publisher = {}
}

@inproceedings{kar60614,
       booktitle = {Sixteenth International Conference on Autonomous Agents and Multiagent Sytems (AAMAS 2017)},
           month = {May},
           title = {Reward Shaping in Episodic Reinforcement Learning},
          author = {Marek Grzes},
       publisher = {ACM},
            year = {2017},
           pages = {565--573},
        keywords = {Reward structures for learning; Multiagent learning; Reward shaping; Reinforcement learning},
             url = {https://kar.kent.ac.uk/60614/},
        abstract = {Recent advancements in reinforcement learning confirm that reinforcement learning techniques can solve large scale problems leading to high quality autonomous decision making. It is a matter of time until we will see large scale applications of reinforcement learning in various sectors, such as healthcare and cyber-security, among others. However, reinforcement learning can be time-consuming because the learning algorithms have to determine the long term consequences of their actions using delayed feedback or rewards. Reward shaping is a method of incorporating domain knowledge into reinforcement learning so that the algorithms are guided faster towards more promising solutions. Under an overarching theme of episodic reinforcement learning, this paper shows a unifying analysis of potential-based reward shaping which leads to new theoretical insights into reward shaping in both model-free and model-based algorithms, as well as in multi-agent reinforcement learning.}
}

@article{10010902373,
author="NG, A. Y.",
title="Policy invariance under reward transformations: Theory and application to reward shaping",
journal="Proceedings of the 16th International Conference on Machine Learning",
ISSN="",
publisher="",
year="1999",
month="",
volume="",
number="",
pages="",
URL="https://ci.nii.ac.jp/naid/10010902373/en/",
DOI="",
}